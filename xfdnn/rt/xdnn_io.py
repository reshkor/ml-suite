#!/usr/bin/env python
#
# // SPDX-License-Identifier: BSD-3-CLAUSE
#
# (C) Copyright 2018, Xilinx, Inc.
#
import numpy as np
# from . import xdnn
import xdnn
import argparse
import os
import json
import math
import cv2
import PyTurboJPEG
from ast import literal_eval as l_eval
import collections
import ntpath

def max_batch_size(x):
    maxb = 16
    if int(x) > maxb:
      print ("Limiting batch size to %d" % maxb)
    x = min( int(x), maxb)   
    return x
  
def extant_file(x):
    """
    'Type' for argparse - checks that file exists but does not open.
    """
    if not os.path.exists(x):
        # Argparse uses the ArgumentTypeError to give a rejection message like:
        # error: argument input: x does not exist
        raise argparse.ArgumentTypeError("{0} does not exist".format(x))
    return x
  
def default_parser_args():
    parser = argparse.ArgumentParser(description='pyXDNN')
    parser.add_argument('--xclbin', help='.xclbin file', required=True, type=extant_file, metavar="FILE")
    parser.add_argument('--batch_sz', type=max_batch_size, default=-1, help='batch size')
    parser.add_argument('--dsp', type=int, default=28, help="xclbin's DSP array width")
    parser.add_argument('--netcfg', help='FPGA instructions generated by compiler for the network',
                        required=True, type=extant_file, metavar="FILE")
    parser.add_argument('--quantizecfg', help="Network's quantization parameters file", 
                        required=True, type=extant_file, metavar="FILE")
    parser.add_argument('--xlnxlib',
        help='FPGA xfDNN lib .so (deprecated)', type=extant_file, metavar="FILE")
    parser.add_argument('--fpgaoutsz', type=int, default=1024,
        help='size of 1 FPGA output blob')
    parser.add_argument('--outsz', type=int, default=1000,
        help='size of last layer\'s output blob')
    parser.add_argument('--datadir',
        help="Folder path to network parameters/weights",
        required=True, type=extant_file, metavar="FILE")
    parser.add_argument('--labels',
        help='result -> labels translation file', type=extant_file, metavar="FILE")
    parser.add_argument('--golden', help='file idx -> expected label file', type=extant_file, metavar="FILE")
    parser.add_argument('--jsoncfg',
        help='json file with nets, data and PEs to use', 
        type=extant_file, metavar="FILE")
    parser.add_argument('--images', nargs='*',
        help='directory or raw image files to use as input', required=True, type=extant_file, metavar="FILE")
    parser.add_argument('--scaleA', type=int, default=10000,
        help='weights scaling value')
    parser.add_argument('--scaleB', type=int, default=30,
        help='activation scaling value ')
    parser.add_argument('--img_raw_scale', type=float, default=255.0,
        help='image raw scale value ')
    parser.add_argument('--img_mean', type=tuple, default=(104.007, 116.669, 122.679),  # BGR for Caffe
        help='image mean values ')
    parser.add_argument('--img_input_scale', type=float, default=1.0,
        help='image input scale value ')
    parser.add_argument('--in_shape', type=tuple, default=(3,224, 224), help='input dimensions')
    parser.add_argument('--zmqpub', default=False, action='store_true',
        help='publish predictions to zmq port 5555')
    parser.add_argument('--perpetual', default=False, action='store_true',
        help='loop over input images forever')
    parser.add_argument('--PE', nargs='?', type=int, default=-1,
        help='preferred PE to run the classification on. Default is auto-select')
    parser.add_argument('--endLayerName', default="",
            help='layer name till the network should be run, helpful for debugging')
    parser.add_argument('--diffStartLayer', type=int, default=0,
            help="if 1 then we can run from any given layer ignoring the X's of first layers")
    parser.add_argument('--v2WeightsFormat', type=bool, default=False,
            help="Weights File specified as KernSizex KernSizey instead of only KernSize, supporting rectangle kernels")
    parser.add_argument('--layerName', default="",
            help='layername until which pyfpga should run, if left default, would run the entire model')
    parser.add_argument('--binaryFormatWeights', type=bool, default=False,
            help="Binary Format Weights Files")
    return parser

def make_dict_args(args):
    def find_all_images(input_dict):
        if 'images' in input_dict and input_dict['images'] is not None:
            inputFiles = []
            for dir_or_image in l_eval(str(input_dict['images'])):
                if os.path.isdir(dir_or_image):
                    inputFiles += [os.path.join(dir_or_image, f) for f in os.listdir(dir_or_image) if os.path.isfile(os.path.join(dir_or_image, f))]
                else:
                    inputFiles += [dir_or_image]
            input_dict['images'] = inputFiles

    def eval_string(input_dict):
        for key, val in list(input_dict.items()):
            try:
                input_dict[key] = l_eval(str(val))
            except:
                pass
            #if val and str(val).isdigit():
            #    input_dict[key] = int(val)

    def ingest_xclbin_json_config(input_dict):
      fname = input_dict['xclbin'] + ".json"
      with open(fname) as data:
        xclbinJson = json.load(data)
        input_dict['overlaycfg'] = xclbinJson

        isV3 = False
        if 'XDNN_VERSION_MAJOR' in xclbinJson \
          and xclbinJson['XDNN_VERSION_MAJOR'] == "3":
          isV3 = True

        if isV3:
          input_dict['xdnnv3'] = True
          libPath = os.environ['LIBXDNN_PATH'] + ".v3"
          if os.path.isfile(libPath):
            os.environ['LIBXDNN_PATH'] = libPath

        if 'XDNN_CSR_BASE' in xclbinJson and input_dict['batch_sz'] == -1:
          csrAddrs = xclbinJson['XDNN_CSR_BASE'].split(",")
          input_dict['batch_sz'] = len(csrAddrs)
          if not isV3:
            input_dict['batch_sz'] *= 2

    args_dict = vars(args)

    find_all_images(args_dict)
    eval_string(args_dict)
    ingest_xclbin_json_config(args_dict)

    if args_dict['jsoncfg']:
        with open(args_dict['jsoncfg']) as jsoncfgFile:
            jsoncfgs = json.load(jsoncfgFile)['confs']
            for jsoncfg in jsoncfgs:
                find_all_images(jsoncfg)
                eval_string(jsoncfg)

                # include all args not in args_dict['jsoncfg'] from original args_dict
                for key, value in list(args_dict.items()):
                    if key not in jsoncfg:
                        jsoncfg[key] = value
            args_dict['jsoncfg'] = jsoncfgs

    return args_dict

def processCommandLine(argv=None):
    """
    Invoke command line parser for command line deployment flows.
    """
    parser = default_parser_args()
    args = parser.parse_args(argv)
    return make_dict_args(args)


# Generic list of image manipulation functions for simplifying preprocess code
def loadImageBlobFromFileScriptBase(imgFile, cmdSeq):
    if isinstance(imgFile, str):
        img = PyTurboJPEG.imread(imgFile)
    else:
        img = imgFile

    orig_shape = img.shape

    for (cmd,param) in cmdSeq:
        #print "command:",cmd,"param:",param
        #print "imshape:",img.shape
        if cmd == 'resize':
            img = cv2.resize(img, (param[0], param[1]))
        elif cmd == 'resize1:1':
            # Currently doesn't work for rectangular output dimensions...
            
            height, width, channels = img.shape
            newdim = max(height, width)
            scalew = float(width) / newdim
            scaleh = float(height) / newdim
            maxdim = max(param)
            neww   = int(maxdim * scalew)
            newh   = int(maxdim * scaleh)
            
            img = cv2.resize(img, (neww, newh))
        elif cmd == 'letterbox':
            height, width, channels = img.shape
            newdim = max(height, width)
            letter_image = np.zeros((newdim, newdim, channels))
            letter_image[:, :, :] = param
            if newdim == width:                
                letter_image[(newdim-height)/2:((newdim-height)/2+height),0:width] = img
            else:
                letter_image[0:height,(newdim-width)/2:((newdim-width)/2+width)] = img
            img = letter_image
            
        elif cmd == 'plot':
            from matplotlib import pyplot as p
            toshow = img.astype(np.uint8)
            if param is not None:
                toshow = np.transpose(toshow, (param[0], param[1], param[2]))
            plt.imshow(toshow, cmap = 'gray', interpolation = 'bicubic')
            plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis
            plt.show()
        elif cmd == 'center_crop':
            size_x = img.shape[1]
            size_y = img.shape[0]
            ll_x = size_x//2 - param[0]//2
            ll_y = size_y//2 - param[1]//2
            img = img[ll_x:ll_x+param[0],ll_y:ll_y+param[1]]            
        elif cmd == 'scale':
            if img.dtype != np.float32:
                img = img.astype(np.float32, order='C')
            if param != 1.0:
                img = img * param
        elif cmd == 'meansub':
            if img.dtype != np.float32:
                img = img.astype(np.float32, order='C')            
            if isinstance(param, np.ndarray):
                img -= param
            else:
                img -= np.array(param, dtype = np.float32, order='C')
        elif cmd == 'chtranspose':
            # HWC->CWH = 2,0,1
            # CWH->HWC = 1,2,0
            img = np.transpose(img, (param[0], param[1], param[2]))
        elif cmd == 'chswap':
            # BGR->RGB = 2,1,0
            # RGB->BGR = 2,1,0
            ch = 3*[None]
            ch[0] = img[0,:,:]
            ch[1] = img[1,:,:]
            ch[2] = img[2,:,:]
            img = np.stack((ch[param[0]],ch[param[1]],ch[param[2]]))

    #    print "final imshape:",img.shape
    return img, orig_shape


# This runs image manipulation script
def loadImageBlobFromFile(imgFile, raw_scale, mean, input_scale, img_h, img_w):
    # Direct resize only
    cmdseqResize = [
        ('resize',(img_h,img_w)),
        ('scale',float(raw_scale)/255),
        ('meansub', mean),
        ('scale', input_scale),
        ('chtranspose',(2,0,1))
        ]

    # Change initial resize to match network training (shown as 256x256->224,224)
    #    cmdseqCenterCrop = [
    #        ('resize',(256,256)),
    #        ('center_crop',(img_h,img_w)),
    #        ('scale',float(raw_scale)/255),
    #        ('meansub', mean),
    #        ('scale', input_scale),
    #        ('chtranspose',(2,0,1))
    #        ]    

    img, orig_shape = loadImageBlobFromFileScriptBase(imgFile, cmdseqResize)
    img = img[ np.newaxis, ...]    

    return img, None


def loadYoloImageBlobFromFile(imgFile, img_h, img_w):
    # This first loads the image
    # letterboxes/resizes
    # divides by 255 to create values from 0.0 to 1.0
    
    # Letter boxing
    # When given a rectangular image
    # If the network expects a square input
    # Reshape the image such that its longer dimension fits exactly in the square
    # i.e.
    #    ----------
    #    |--------|
    #    | IMAGE  |
    #    |--------|
    #    ----------

    cmdseqYolov2 = [
        ('resize1:1',(img_w,img_h)),
        ('scale',(1.0/255.0)),
        ('letterbox',(0.5)),
        ('chtranspose',(2,0,1)),
        ('chswap',(2,1,0))
        ]

    
    img, orig_shape = loadImageBlobFromFileScriptBase(imgFile, cmdseqYolov2)
    img = img[ np.newaxis, ...]    
    return img, orig_shape


def getFilePaths(paths_list):
    img_paths = []
    for p in paths_list:
        if os.path.isfile(p):
            img_paths.append( os.path.abspath(p) )
        else:
            for dirpath,_,filenames in os.walk(p):
                for f in filenames:
                    img_paths.append( os.path.abspath(os.path.join(dirpath, f)))

    return img_paths


def getTopK(output, labels, topK):
    topKIdx = np.argsort(output)[-topK:]
    topKVals = [output[ti] for ti in topKIdx]
    topKList = zip( topKVals, topKIdx )
    topKList.reverse()
    return [(topKList[j][0], labels[topKList[j][1]]) for j in range(topK)]

def getGoldenMap(goldenFile):
    goldenMap = collections.OrderedDict()
    with open(goldenFile, 'r') as f:
        for line in f:
            fname = line[:line.rfind(' ')]
            goldenIdx = int(line[line.rfind(' ')+1:])
            goldenMap[fname] = goldenIdx

    return goldenMap

def isTopK ( out, goldenMap, fileName, labels, topK = 5):
    f = ntpath.basename(fileName)
    topKs = getTopK(out, labels, topK)
    for (_, label) in topKs:
        if ( label == labels[goldenMap[f]]):
            return True

    return False

def get_labels (label_file):
  labels = None
  if (label_file):
    with open(label_file, 'r') as f:
      labels = [line.strip() for line in f]

  return labels

def printClassification(output, img_paths, labels, topK = 5):
  if labels is not None:
    print ( getClassification ( output, img_paths, labels, topK))

def getClassification(output, img_paths, labels, topK = 5, zmqPub = False):
  """
  Print the result of classification given class scores, and a synset labels file.

  :param output: Class scores, typically the output of the softmax layer.
  :type output: numpy.ndarray.
  :param img_paths: list of path(s) to image(s)
  :param label_file: path to label file
  :type args: dict.
  """
  ret = ""
  if not isinstance(img_paths, list):
    img_paths = [img_paths]

  for i,p in enumerate(img_paths):
    topXs = getTopK(output[i,...], labels, topK)
    inputImage = "for {:s} ".format(p if isinstance(p, str) else 'raw_input')
    if zmqPub :
      ret += (img_paths[i] + '\n')
    else :
      ret += "---------- Prediction {:d}/{:d} {:s}----------\n".format(i+1, output.shape[0], inputImage)
    for (prob, label) in topXs:
      ret += ("{:.4f} \"{:s}\"\n".format(prob, label))

  return ret

def XDLFloadWeights ( args,Weights,outChans,inChans,kernH,kernW,layerName, isxdnnv3=False) :
  print('Loading weights/bias/quant_params to FPGA...')

  if isxdnnv3:
    size = xdnn.v3computeWeightsBiasQuantSize(kernW, kernH, outChans, int(math.ceil(float(inChans)/float(96))), 0, 0, False)
    size=size*2
  else:
    size = xdnn.computeWeightsBiasQuantSize(\
          kernW,kernH,inChans,outChans,True if args['quantizecfg'] else False)

  blob = xdnn.makeWeightsBiasQuantBlob(size)
  bias=[0 for v in range(outChans)]
  if isxdnnv3:
    offset = xdnn.v3fillWeightsBiasQuantBlob(blob, 0,
          args['quantizecfg'], Weights, args['scaleA'], bias, args['scaleB'],
          kernW,kernH,inChans,outChans,layerName)
  else:
    offset = xdnn.fillWeightsBiasQuantBlob(blob, 0,
          args['quantizecfg'], Weights, args['scaleA'], bias, args['scaleB'],
          kernW,kernH,inChans,outChans,layerName)
  fps = xdnn.loadBlobToDdr(blob, size, int(args['PE']))

  return (blob, fps)

def XDLFBunchComputeSizeLatestRepl(args, outChans, inChans, kernH, kernW,srcFullSectNum, srcReplSectNum, srcReplUnitNum, isxdnnv3=False):
  is8Bit=True
  if isxdnnv3:
    size = xdnn.v3computeWeightsBiasQuantSize(kernW, kernH, outChans, int(srcFullSectNum), int(srcReplSectNum), int(srcReplUnitNum), True)
    size=size*2
  else:
    size = xdnn.computeWeightsBiasQuantSize(\
          kernW,kernH,inChans,outChans,True if args['quantizecfg'] else False)

  return size

def loadWeights_samePE (args) :
    """
    Load weights to off chip device memory. The weights are first quantized.

    :param args: Collection of arguments. Most importanly args["datadir"] which is the path to a folder containing weights & biases.
    :type args: dict.
    :returns: tuple -- (weightsBlob, fcWeight, fcBias) -- <class 'xdnn.LP_c_short'>, numpy.ndarray, numpy.ndarray
    """

    if not isinstance(args, list):
        args = [args]
    else:
        PEs = [arg['PE'] for arg in args]
        if PEs[1:] != PEs[:-1]:
            raise RuntimeError('Trying to load weights on multiple PEs simultaneously.')

    if 'xdnnv3' in args and args['xdnnv3']:
        raise NotImplementedError("V3 functionality not implemented yet")
    else:
        weightBlob = loadWeightsBiasQuant_samePE(args)

    weightsBlob = {}
    fcWeight    = {}
    fcBias      = {}
    for arg in args:
        name = str(arg['name'])
        weightsBlob[name] = weightBlob
        (fcWeight[name], fcBias[name]) = loadFCWeightsBias(arg)
    return (weightsBlob, fcWeight, fcBias)

def XDLFloadWeights (args, Weights, outChans, inChans, kernH, kernW, layerName, isxdnnv3=False) :
    print("Loading weights/bias/quant_params to FPGA...")

    if isxdnnv3 != "False":
        size = xdnn.v3computeWeightsBiasQuantSize(kernW, kernH, outChans, int(math.ceil(float(inChans) / float(96))), 0, 0, False)
        size = size * 2
    else:
        size = xdnn.computeWeightsBiasQuantSize(\
                    kernW, kernH, inChans, outChans, True if args['quantizecfg'] else False)

    blob = xdnn.makeWeightsBiasQuantBlob(size)
    bias = [0 for v in range(outChans)]
    if isxdnnv3 != "False":
        offset = xdnn.v3fillWeightsBiasQuantBlob(blob, 0,
                    args['quantizecfg'], Weights, args['scaleA'], bias, args['scaleB'],
                    kernW, kernH, inChans, outChans, layerName)
    else:
        offset = xdnn.fillWeightsBiasQuantBlob(blob, 0,
                    args['quantizecfg'], Weights, args['scaleA'], bias, args['scaleB'],
                    kernW, kernH, inChans, outChans, layerName)
    layer2OffsetMap = "%s:%d" % (layerName, 0)
    fps = xdnn.loadBlobToDdr(blob, size, layer2OffsetMap, int(args['PE']))

    return (blob, fps)


def XDLFBunchComputeSizeLatestRepl(args, outChans, inChans, kernH, kernW, srcFullSectNum, srcReplSectNum, srcReplUnitNum, isxdnnv3=False):

    if isxdnnv3:
        size = xdnn.v3computeWeightsBiasQuantSize(kernW, kernH, outChans, int(srcFullSectNum), int(srcReplSectNum), int(srcReplUnitNum), False)
        size = size * 2
    else:
        size = xdnn.computeWeightsBiasQuantSize(\
                    kernW, kernH, inChans, outChans, True if args['quantizecfg'] else False)

    return size


def XDLFBunchloadWeightsBiasQuantLatestRepl(args, allLayerNames, allLayersWeightsBiasQuantizeKey, allConvLayerNamesParams, size, isxdnnv3):
    print("Loading weights/bias/quant_params to FPGA...")

    blob = xdnn.makeWeightsBiasQuantBlob(size)

    offset = 0

    layer2OffsetMapStr = ""
    for layerName in allLayerNames:
        if layer2OffsetMapStr != "":
            layer2OffsetMapStr += ","
        layer2OffsetMapStr += "%s:%d" % (layerName, offset)

        if isxdnnv3:
            offset += xdnn.v3fillWeightsBiasQuantBlob(blob, offset,
                    args['quantizecfg'], allLayersWeightsBiasQuantizeKey[layerName]['weights'], args['scaleA'], allLayersWeightsBiasQuantizeKey[layerName]['Bias'], args['scaleB'],
                 allLayersWeightsBiasQuantizeKey[layerName]['kernW'], allLayersWeightsBiasQuantizeKey[layerName]['kernH'], allLayersWeightsBiasQuantizeKey[layerName]['inChans'], allLayersWeightsBiasQuantizeKey[layerName]['outChans'], int(allConvLayerNamesParams[layerName]['srcFullSectNum']), int(allConvLayerNamesParams[layerName]['srcReplSectNum']), int(allConvLayerNamesParams[layerName]['srcReplUnitNum']), int(allConvLayerNamesParams[layerName]['srcReplUnitWidth']), int(allConvLayerNamesParams[layerName]['convHalfRateMode']), layerName)
        else:
            offset += xdnn.fillWeightsBiasQuantBlob(blob, offset,
                    args['quantizecfg'], allLayersWeightsBiasQuantizeKey[layerName]['weights'], args['scaleA'], allLayersWeightsBiasQuantizeKey[layerName]['Bias'], args['scaleB'],
                 allLayersWeightsBiasQuantizeKey[layerName]['kernW'], allLayersWeightsBiasQuantizeKey[layerName]['kernH'], allLayersWeightsBiasQuantizeKey[layerName]['inChans'], allLayersWeightsBiasQuantizeKey[layerName]['outChans'], layerName)
    fps = xdnn.loadBlobToDdr(blob, size, layer2OffsetMapStr, int(args['PE']))

    return (blob, fps)


def XDLFBunchComputeSize(args, outChans, inChans, kernH, kernW, isxdnnv3=False):

    if isxdnnv3:
        size = xdnn.v3computeWeightsBiasQuantSize(kernW, kernH, outChans, int(math.ceil(float(inChans) / float(96))), 0, 0, False)
        size = size * 2
    else:
        size = xdnn.computeWeightsBiasQuantSize(\
                    kernW, kernH, inChans, outChans, True if args['quantizecfg'] else False)

    return size


def XDLFBunchloadWeightsBiasQuant(args, allLayerNames, allLayersWeightsBiasQuantizeKey, size, isxdnnv3):
    print("Loading weights/bias/quant_params to FPGA...")

    print("MNDBG total A size {:d}".format(size))
    blob = xdnn.makeWeightsBiasQuantBlob(size)

    layer2OffsetMapStr = ""
    offset = 0

    for layerName in allLayerNames:
        if layer2OffsetMapStr != "":
            layer2OffsetMapStr += ","
        layer2OffsetMapStr += "%s:%d" % (layerName, offset)

        if isxdnnv3:
            offset += xdnn.v3fillWeightsBiasQuantBlob(blob, offset,
                    args['quantizecfg'], allLayersWeightsBiasQuantizeKey[layerName]['weights'], args['scaleA'], allLayersWeightsBiasQuantizeKey[layerName]['Bias'], args['scaleB'],
                 allLayersWeightsBiasQuantizeKey[layerName]['kernW'], allLayersWeightsBiasQuantizeKey[layerName]['kernH'], allLayersWeightsBiasQuantizeKey[layerName]['inChans'], allLayersWeightsBiasQuantizeKey[layerName]['outChans'], layerName)
        else:
            offset += xdnn.fillWeightsBiasQuantBlob(blob, offset,
                    args['quantizecfg'], allLayersWeightsBiasQuantizeKey[layerName]['weights'], args['scaleA'], allLayersWeightsBiasQuantizeKey[layerName]['Bias'], args['scaleB'],
                 allLayersWeightsBiasQuantizeKey[layerName]['kernW'], allLayersWeightsBiasQuantizeKey[layerName]['kernH'], allLayersWeightsBiasQuantizeKey[layerName]['inChans'], allLayersWeightsBiasQuantizeKey[layerName]['outChans'], layerName)
    fps = xdnn.loadBlobToDdr(blob, size, layer2OffsetMapStr, int(args['PE']))

    return (blob, fps)

# def loadWeightsBiasQuant_samePE(args_list):
#     print("Loading weights/bias/quant_params to FPGA...")
#
#     size = 0
#     for args in args_list:
#         fi = 0
#         while True:
#             fname = "%s/wbq_size_%d" % (args['datadir'], fi)
#             if not os.path.isfile(fname):
#                 break
#
#             with open(fname, 'r') as f:
#                 data = f.read()
#                 vals = data.strip().split(' ')
#                 vals = [int(v) for v in vals]
#                 if 'v2WeightsFormat' in args and args['v2WeightsFormat'] == 1:
#                     size += xdnn.computeWeightsBiasQuantSize(\
#                         vals[0], vals[1], vals[2], vals[3], True if args['quantizecfg'] else False)
#                 else:
#                     size += xdnn.computeWeightsBiasQuantSize(\
#                         vals[0], vals[0], vals[1], vals[2], True if args['quantizecfg'] else False)
#
#             fi += 1
#
#     # print("ANDBG total A size {:d}".format(size))
#     blob = xdnn.makeWeightsBiasQuantBlob(size)
#
#     layer2OffsetMapStr = ""
#     offset = 0
#     for args in args_list:
#         fi = 0
#         while True:
#             fname = "%s/fwbqb_%d" % (args['datadir'], fi)
#             if 'binaryFormatWeights' in args:
#                 if args['binaryFormatWeights'] == 1:
#                     fname = "%s/fwbqbbinary_%d.bin" % (args['datadir'], fi)
#             if not os.path.isfile(fname):
#                 break
#
#             layerName = None
#             weights   = None
#             bias      = None
#             if 'v2WeightsFormat' in args and args['v2WeightsFormat'] == 1:
#                 kernWidth  = None
#                 kernHeight = None
#             else:
#                 kern = None
#             inChans  = None
#             outChans = None
#
#             if 'binaryFormatWeights' in args:
#                 if args['binaryFormatWeights'] == 1:
#                     if 'v2WeightsFormat' in args and args['v2WeightsFormat'] != 1:
#                         print("Weights should be given in the v2WeightsFormat, KernSizex and KernSizey. If they are already\
#                             in v2WeightsFormat, then set the argument v2WeightsFormat to 1")
#                         sys.exit(1)
#                     vals = []
#                     (layerName, kernWidth, kernHeight, inChans, outChans, vals) = xdnn.readWeightsFile(fname)
#                     weights = [float(v) for v in vals]
#                 elif args['binaryFormatWeights'] == 0:
#                     with open(fname, 'r') as f:
#                         data = f.read()
#                         vals = data.strip().split(' ')
#                         layerName = vals[0]
#                         if 'v2WeightsFormat' in args and args['v2WeightsFormat'] == 1:
#                             kernWidth   = int(vals[1])
#                             kernHeight  = int(vals[2])
#                             inChans     = int(vals[3])
#                             outChans    = int(vals[4])
#                             weights     = [float(v) for v in vals[5:]]
#                         else:
#                             kern        = int(vals[1])
#                             inChans     = int(vals[2])
#                             outChans    = int(vals[3])
#                             weights     = [float(v) for v in vals[4:]]
#             else:
#                 with open(fname, 'r') as f:
#                     data = f.read()
#                     vals = data.strip().split(' ')
#                     layerName = vals[0]
#                     if 'v2WeightsFormat' in args and args['v2WeightsFormat'] == 1:
#                         kernWidth   = int(vals[1])
#                         kernHeight  = int(vals[2])
#                         inChans     = int(vals[3])
#                         outChans    = int(vals[4])
#                         weights     = [float(v) for v in vals[5:]]
#                     else:
#                         kern        = int(vals[1])
#                         inChans     = int(vals[2])
#                         outChans    = int(vals[3])
#                         weights     = [float(v) for v in vals[4:]]
#
#             fname = "%s/fwbqb_bias_%d" % (args['datadir'], fi)
#
#             with open(fname, 'r') as f:
#                 data = f.read()
#                 vals = data.strip().split(' ')
#                 if 'v2WeightsFormat' in args:
#                     if args['v2WeightsFormat'] == 1:
#                         vals = vals[5:]
#                     elif args['v2WeightsFormat'] == 0:
#                         vals = vals[4:]
#                 else:
#                     vals = vals[4:]
#                 bias = [float(v) for v in vals]
#
#             if layer2OffsetMapStr != "":
#                 layer2OffsetMapStr += ","
#             layer2OffsetMapStr += "%s:%d" % (layerName, offset)
#
#             # print("ANDBG {:d} {:s} {:d} {:d} {:d}".format(offset, layerName, fi, len(weights), len(bias)))
#             if 'v2WeightsFormat' in args and args['v2WeightsFormat'] == 1:
#                 offset += xdnn.fillWeightsBiasQuantBlob(blob, offset,
#                     args['quantizecfg'], weights, args['scaleA'], bias, args['scaleB'],
#                     kernWidth, kernHeight, inChans, outChans, layerName)
#             else:
#                 offset += xdnn.fillWeightsBiasQuantBlob(blob, offset,
#                     args['quantizecfg'], weights, args['scaleA'], bias, args['scaleB'],
#                     kern, kern, inChans, outChans, layerName)
#
#             fi += 1
#     xdnn.loadBlobToDdr(blob, size, layer2OffsetMapStr, int(args_list[0]['PE']))
#
#     return blob

def getNearFileMatchWithPrefix(path, prefix):
    nearMatches = [f for f in os.listdir(path) if f.startswith(prefix)]
    nearMatches.sort()
    if len(nearMatches) > 0:
        return "%s/%s" % (path, nearMatches[0])

    return None


def loadFCWeightsBias(arg):
  data_dir = arg['datadir']

  if data_dir:
    fname = "%s/fc" % data_dir
    if not os.path.exists(fname):
      nearMatch = getNearFileMatchWithPrefix(data_dir, "fc")
      if nearMatch:
        fname = nearMatch
    if os.path.exists(fname):
      with open(fname, 'r') as f:
        line = f.read()
        vals = line.strip().split(' ')
        weight = [float(v) for v in vals]
    else:
      print("No FC layers found in {:s}".format(data_dir))
      return (None, None)

    fname = "%s/fc_bias" % data_dir
    if not os.path.exists(fname):
      nearMatch = getNearFileMatchWithPrefix(data_dir, "fc_bias")
      if nearMatch:
        fname = nearMatch
    with open(fname, 'r') as f:
      line = f.read()
      vals = line.strip().split(' ')
      bias = [float(v) for v in vals]
  else:
    # no datadir provided; load dummy weights
    weightsSize = arg['fpgaoutsz']*arg['outsz']
    weight = [0] * weightsSize
    bias = [0] * arg['outsz']

  return (np.asarray(weight, dtype=np.float32), np.asarray(bias, dtype=np.float32))

# def default_rt_args():
        # parser = argparse.ArgumentParser()


def default_compiler_args():
        parser = argparse.ArgumentParser()
        parser.add_argument("--memory", type=int, default=8)
        parser.add_argument("--generatefile", type=str)
        parser.add_argument("--noreplication", default=True, action='store_true')
        parser.add_argument("--godreplication", type=str, default=None)
        parser.add_argument("--forceweights", type=str, default=None)
        parser.add_argument("--parallelismstrategy", type=str, default="['bottom', 'tops']")
        parser.add_argument("--parallelread", type=str, default=None)
        parser.add_argument("--deephifilename", type=str, default=None)
        parser.add_argument("--dsp", type=int, default=28)
        parser.add_argument("--ddr", type=int, default=4096)
        parser.add_argument("--versionjson", type=str, default=None)
        parser.add_argument("--fromtensorflow", default=True)
        parser.add_argument("--weights", default=False)
        parser.add_argument("--strategy", type=str, default="all")
        parser.add_argument("--schedulefile", type=str)
        parser.add_argument("--pngfile", type=str)
        parser.add_argument("--rankdir", type=str, default='BT')
        parser.add_argument("--bytesperpixels", type=int, default=1)
        parser.add_argument("--finalnode", type=str, default=None)
        parser.add_argument("--parallelism", default=False, action='store_true')
        parser.add_argument("--manualdeconv", default=False, action='store_true')
        parser.add_argument("--manasadebugmode", default=False, action='store_true')
        parser.add_argument("--nodynamicscaling", default=False, action='store_true')
        parser.add_argument("--cpulayermustgo", default=False, action='store_true')
        parser.add_argument("--poolingaround", default=False, action='store_true')
        parser.add_argument("--conv_1x1_s2", default=False, action='store_true')
        parser.add_argument("--bridges", default=False, action='store_true')
        parser.add_argument("--pipelineconvmaxpool", default=False, action='store_true')
        parser.add_argument("--manualbatch", default=False, action='store_true')
        parser.add_argument("--networkfile", type=str, required=True)
        parser.add_argument("--placeholdershape", type=str, default=None)
        parser.add_argument("--verbose", default=False, action='store_true')
        parser.add_argument("--lasttensorbyname", type=str, default=None)
        parser.add_argument("--concatstrategy", type=str, default=None)
        parser.add_argument("--dedicateddsp", default=False, action='store_true')
        parser.add_argument('--xdnnv3', default=False, action='store_true',
                            help='is xdnnv3')
        parser.add_argument('--anew', type=str, default=None, action='store',
                            help="prefix of the new prototext, we rewrite \
                            model and prototext CAFFE only")
        parser.add_argument('--savepickle', type=str, default=None,
                            action='store', help="Save the pydot graph as \
                            pickle")

        parser.add_argument("--device", type=str, required=True, choices=['CPU', 'FPGA', 'HWEmu'])
        return parser

