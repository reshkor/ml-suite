{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xilinx ML Developer Lab\n",
    "\n",
    "## Introduction\n",
    "Welcome to Xilinx's Virtual ML Suite Developer Lab. By completing this lab, you will gain valuable hands on experience in using the Xilinx ML Suite. This lab covers how to use Python APIs to deploy the included models, as well as how to compile and quantize custom models with the xfDNN Python tools.\n",
    "\n",
    "Object Detection w/ YOLOv2 (Darknet -> Caffe)\n",
    "This tutorial demonstrates the steps required to prepare and deploy a trained Darknet model for FPGA acceleration through the ML Suite. We will prepare a trained YOLO v2 model, and then run a single detection. \n",
    "\n",
    "For more information on the object detection model, Yolo, [please read more here][]. \n",
    "\n",
    "For more information on the ML Suite please visit [Xilinx.com/ml][] and [here][]\n",
    "\n",
    "To use the Xilinx ML Suite for your own applications, please visit our [Github][]\n",
    "\n",
    "Are you new to Jupyter Notebooks? Learn more here:  \n",
    "[Basic Tutorial][]   \n",
    "[Advanced Features][]    \n",
    "\n",
    "[please read more here]: yolo-background.md\n",
    "\n",
    "[Basic Tutorial]: https://github.com/Xilinx/PYNQ/blob/master/docs/source/jupyter_notebooks.ipynb\n",
    "[Advanced Features]: https://github.com/Xilinx/PYNQ/blob/master/docs/source/jupyter_notebooks_advanced_features.ipynb\n",
    "[Github]: https://github.com/Xilinx/ml-suite\n",
    "[Xilinx.com/ml]: https://www.xilinx.com/ml\n",
    "[here]: https://github.com/Xilinx/ml-suite/blob/master/docs/tutorials/ml-suite-overview.md\n",
    "\n",
    "\n",
    "## Lab Overview \n",
    "The Lab is divided into 4 parts:   \n",
    "**Part 1**: Darknet to Caffe conversion   \n",
    "**Part 2**: Compile The Model   \n",
    "**Part 3**: Quantize The Model  \n",
    "**Part 4**: Deploy The Model  \n",
    "\n",
    "\n",
    "**Ready? Lets begin!**  \n",
    "\n",
    "## Part 1: Darknet to Caffe conversion\n",
    "\n",
    "### 1. Import required packages and check environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,cv2,timeit\n",
    "from __future__ import print_function\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Bring in Xilinx ML-Suite Compiler, Quantizer, PyXDNN\n",
    "from xfdnn.tools.compile.bin.xfdnn_compiler_caffe import CaffeFrontend as xfdnnCompiler\n",
    "from xfdnn.tools.quantize.quantize import CaffeFrontend as xfdnnQuantizer\n",
    "#import xfdnn.rt.xdnn as pyxfdnn\n",
    "#import xfdnn.rt.xdnn_io as pyxfdnn_io\n",
    "import xfdnn.rt.xdnn as pyxfdnn\n",
    "import xfdnn.rt.xdnn_io as pyxfdnn_io\n",
    "\n",
    "# Bring in darknet2caffe functions\n",
    "from darknet2caffe import *\n",
    "\n",
    "# Bring in Non-Max Suppression \n",
    "import nms\n",
    "\n",
    "# Bring in utility for drawing boxes\n",
    "from yolo_utils import generate_colors, draw_boxes\n",
    "\n",
    "# Ignore some warnings from the quantizer\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "print(\"Current working directory: %s\" % os.getcwd())\n",
    "print(\"Running on host: %s\" % os.uname()[1])\n",
    "print(\"Running w/ LD_LIBRARY_PATH: %s\" %  os.environ[\"LD_LIBRARY_PATH\"])\n",
    "print(\"Running w/ XILINX_OPENCL: %s\" %  os.environ[\"XILINX_OPENCL\"])\n",
    "print(\"Running w/ XCLBIN_PATH: %s\" %  os.environ[\"XCLBIN_PATH\"])\n",
    "print(\"Running w/ PYTHONPATH: %s\" %  os.environ[\"PYTHONPATH\"])\n",
    "print(\"Running w/ SDACCEL_INI_PATH: %s\" %  os.environ[\"SDACCEL_INI_PATH\"])\n",
    "print(\"Running w/ LIBXDNN_PATH: %s\" %  os.environ[\"LIBXDNN_PATH\"])\n",
    "\n",
    "!whoami \n",
    "\n",
    "# Make sure there is no error in this cell\n",
    "# The xfDNN runtime depends upon the above environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use a config dictionary to pass parameters and select an Image\n",
    "\n",
    "First, we will setup and use a config dictionary to simplify handling of the arguments. For this example, we have a number of images to select from. Select one from the widget below. Image courtesy of openimages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config[\"platform\"] = \"aws\"\n",
    "    \n",
    "# This is a widget that allows you to select an image from the calibration_directory.\n",
    "# Run this and a dropdown will be presented with the selection of all the images from the calibration_directory.\n",
    "# Select one and run the next block of code and it will grab the value.\n",
    "#from ipywidgets import interact, interactive, interact_manual\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "calibration_path = \"../../xfdnn/tools/quantize/calibration_directory/\"\n",
    "img_list = os.listdir(calibration_path)\n",
    "img_list.reverse()\n",
    "img = None\n",
    "\n",
    "def setConfigImage(image):\n",
    "    global img\n",
    "    img = calibration_path+image\n",
    "    \n",
    "interact(setConfigImage,image=img_list,)\n",
    "config[\"images\"] = [img]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"images\"] = [img]\n",
    "\n",
    "img = cv2.imread(config[\"images\"][0])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.rcParams['figure.figsize'] = [24.0,16.0]\n",
    "plt.imshow(img)\n",
    "plt.title(config[\"images\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run darknet2caffe script   \n",
    "\n",
    "Here, we need to convert the model from darknet to caffe. The below cell will create a prototxt, and a caffemodel in the ./work directory. Such a translation will be necessary for any model defined in darknet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the darknet2caffe python script which will convert a yolov2 darknet model to a caffe model\n",
    "# This script additionally will remove batchnorm layers, by merging the parameters\n",
    "# This step is intentionally performed upstream of the compiler due to Darknet's non-conforming implementation of \n",
    "#   batch normalization\n",
    "\n",
    "config[\"prototxt\"] = \"./work/yolov2.prototxt\" \n",
    "config[\"caffemodel\"] = \"./work/yolov2.caffemodel\"\n",
    "\n",
    "darknet2caffe(\"../models/darknet/yolov2/fp32/yolo.xdnn.nobn.608.cfg\",\n",
    "              \"../models/darknet/yolov2/fp32/yolo.xdnn.nobn.weights\",\n",
    "              config[\"prototxt\"],\n",
    "              config[\"caffemodel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Compile The Model\n",
    "\n",
    "In this part, you will learn what steps are required to prepare and compile a network and model. Before being able to deploy networks/models to Xilinx FPGAs you will need to compile them.  \n",
    "\n",
    "This step is more than just converting the framework graph representation to one Xilinx can execute. The xfDNN Compiler is a high performance optimizer for Machine Learning inference. Some of the techniques it performs are fusing and merging layers, optimizing memory usage and pre-scheduling complete network deployment. These techniques increase inference rates and lower inference latency. \n",
    "\n",
    "Using the xfDNN Compiler is an offline process, which only needs to be performed once per network. As you will see, the process is simple and quick. \n",
    "\n",
    "\n",
    "### 4. Define an xfdnnCompiler instance and pass it arguments    \n",
    "To simplify handling of arguments, we continue to use a config dictionary. Take a look at the dictionary entries below. \n",
    "\n",
    "The arguments that need to be passed are: \n",
    "- `prototxt` - Caffe representation of the network\n",
    "- `caffemodel` - Pre-trained Model for the network \n",
    "- `outmodel` - Filename for saving the prototxt and caffemodel of the optimized network\n",
    "- `netcfg` - Filename to save micro-instruction produced by the compiler needed to deploy\n",
    "- `memory` - Parameter to set the on-chip memory for the target xDNN overlay. This example will target an overlay with 5 MB of cache. \n",
    "- `dsp` - Parameter to set the size of the target xDNN overlay. This example uses an overlay of size 32x56 DSPs. \n",
    "\n",
    "**Please Note:**  \n",
    "Memory, and DSP are critical arguments that correspond to the hardware accelerator you plan to load onto the FPGA.  \n",
    "To learn more about the available overlays and these parameters, please visit the [Overlay Selector Guide][]. \n",
    "  \n",
    "\n",
    "The xfDNN Compiler interfaces with Caffe to read a network graph, and generates a sequence of instructions for the xfDNN Deploy APIs to execute on the FPGA.  \n",
    "\n",
    "During this process the xfDNN Compiler performs computational graph traversal, node merging and optimization, memory allocation and optimization and, finally, micro-instruction generation.\n",
    "\n",
    "[Overlay Selector Guide]: https://github.com/Xilinx/ml-suite/blob/master/overlaybins/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compiler Arguments\n",
    "#config[\"prototxt\"] = \"./work/yolov2.prototxt\" \n",
    "#config[\"caffemodel\"] = \"./work/yolov2.caffemodel\"\n",
    "config[\"outmodel\"] = \"../work/optimized_model\" # String for naming optimized prototxt, caffemodel\n",
    "config[\"netcfg\"] = \"../work/fpga.cmds\" # Compiler will generate FPGA instructions\n",
    "config[\"memory\"] = 5 # Available on-chip SRAM\n",
    "config[\"dsp\"] = 56 # Width of Systolic Array\n",
    "\n",
    "compiler = xfdnnCompiler(\n",
    "    networkfile=config[\"prototxt\"],       # Prototxt filename: input file\n",
    "    weights=config[\"caffemodel\"],         # Floating Point Weights: input file\n",
    "    anew=config[\"outmodel\"],              # String for intermediate prototxt/caffemodel\n",
    "    generatefile=config[\"netcfg\"],        # Script filename: output file\n",
    "    memory=config[\"memory\"],              # Available on chip SRAM within xclbin\n",
    "    dsp=config[\"dsp\"]                     # Rows in DSP systolic array within xclbin\n",
    ")\n",
    "\n",
    "# Invoke compiler\n",
    "try:\n",
    "    compiler.compile()\n",
    "\n",
    "    # The compiler extracts the floating point weights from the .caffemodel. \n",
    "    # This weights dir will be stored in the work dir with the appendex '_data'. \n",
    "    # The compiler will name it after the caffemodel, and append _data\n",
    "    config[\"datadir\"] = \"../work/\" + config[\"caffemodel\"].split(\"/\")[-1]+\"_data\"\n",
    "        \n",
    "    if os.path.exists(config[\"datadir\"]) and os.path.exists(config[\"netcfg\"]+\".json\"):\n",
    "        print(\"Compiler successfully generated JSON and the data directory: %s\" % config[\"datadir\"])\n",
    "    else:\n",
    "        print(\"Compiler failed to generate the JSON or data directory: %s\" % config[\"datadir\"])\n",
    "        raise\n",
    "        \n",
    "    print(\"**********\\nCompilation Successful!\\n\")\n",
    "    \n",
    "    import json\n",
    "    data = json.loads(open(config[\"netcfg\"]+\".json\").read())\n",
    "    print(\"Network Operations Count: %d\"%data['ops'])\n",
    "    print(\"DDR Transfers (bytes): %d\"%data['moveops']) \n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Failed to complete compilation:\",e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Quantize The Model\n",
    "\n",
    "In this part of the lab, we will look at quantizing 32-bit floating point models to Int16 or Int8 inpreparation for deployment. Deploying Int16/8 models dramatically improves inference deployment and lowers latency. While floating point precision is useful in model training, it is more energy efficient, as well as, lower latency to deploy models in lower precison.\n",
    "\n",
    "The xfDNN Quantizer performs a technique of quantization known as recalibration. This technique does not require full retraining of the model, and can be accomplished in a matter of seconds, as you will see below. It also allows you to maintain the accuracy of the high precision model.\n",
    "\n",
    "Quantization of the model does not alter the orginal high precision model, rather, it calculates the dynamic range of the model and produces scaling parameters recorded in a json file, which will be used by the xDNN overlay during execution of the network/model. Quantization of the model is an offline process that only needs to be performed once per model. The quantizer produces an optimal target quantization from a given network (prototxt and caffemodel) and calibration set (unlabeled input images) without requiring hours of retraining or a labeled dataset.  \n",
    "\n",
    "\n",
    "### 5. Create Quantizer Instance and pass arguments\n",
    "\n",
    "To simplify handling of arguments, a config dictionary is used. Take a look at the dictionary below.\n",
    "\n",
    "The arguments that need to be passed are:\n",
    "- `deploy_model` - Filename generated by the compiler for the optimized prototxt and caffemodel.\n",
    "- `quantizecfg` - Output JSON filename of quantization scaling parameters. \n",
    "- `bitwidths` - Desired precision from quantizer. This is to set the precision for [image data, weight bitwidth, conv output]. All three values need to be set to the same setting. The valid options are `16` for Int16 and `8` for Int8.  \n",
    "- `in_shape` - Sets the desired input image size of the first layer. Images will be resized to these demensions and must match the network data/placeholder layer.\n",
    "- `transpose` - Images start as H,W,C (H=0,W=1,C=2) transpose swaps to C,H,W (2,0,1) for typical networks.\n",
    "- `channel_swap` - Depending on network training and image read, can swap from RGB (R=0,G=1,B=2) to BGR (2,1,0).\n",
    "- `raw_scale` - Depending on network training, scale pixel values before mean subtraction.\n",
    "- `img_mean` - Depending on network training, subtract image mean if available.\n",
    "- `input_scale` - Depending on network training, scale after subtracting mean.\n",
    "- `calibration_size` - Number of images the quantizer will use to calculate the dynamic range. \n",
    "- `calibration_directory` - Location of dir of images used for the calibration process. \n",
    "\n",
    "Below is an example with all the parameters filled in. `channel_swap` `raw_scale` `img_mean` `input_scale` are image preprocessing arguments specific to a given model.\n",
    "\n",
    "**Note:** This may take a couple of minutes to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use a config dictionary to pass parameters to the compiler\n",
    "# Quantizer Arguments\n",
    "#config[\"outmodel\"] = Defined in Step 4 # String for naming intermediate prototxt, caffemodel\n",
    "config[\"quantizecfg\"] = \"../work/quantization_params.json\" # Quantizer will generate quantization params\n",
    "config[\"bitwidths\"] = [16,16,16] # Supported quantization precision\n",
    "config[\"in_shape\"] = [3,608,608] # Images will be resized to this shape -> Needs to match prototxt\n",
    "config[\"transpose\"] = [2,0,1] # (H,W,C)->(C,H,W) transpose argument to quantizer\n",
    "config[\"channel_swap\"] = [2,1,0] # (R,G,B)->(B,G,R) Channel Swap argument to quantizer\n",
    "config[\"raw_scale\"] = 1.0 # Yolo effectively divides by 255 (Quantizer will represent image as 0 <-> 1 which is equiv)\n",
    "config[\"img_mean\"] = [0.0, 0.0, 0.0] # Mean of the training set used for mean subtraction (Not performed for yolo)\n",
    "config[\"input_scale\"] = 1.0 # Input multiplier, Images are scaled by this factor after mean subtraction\n",
    "config[\"calibration_size\"] = 15 # Number of calibration images quantizer will use\n",
    "config[\"calibration_directory\"] = \"../../xfdnn/tools/quantize/calibration_directory\" # Directory of images\n",
    "\n",
    "quantizer = xfdnnQuantizer(\n",
    "    deploy_model=config[\"outmodel\"]+\".prototxt\",        # Model filename: input file\n",
    "    weights=config[\"outmodel\"]+\".caffemodel\",           # Floating Point weights\n",
    "    output_json=config[\"quantizecfg\"],                    # Quantization JSON output filename\n",
    "    bitwidths=config[\"bitwidths\"],                        # Fixed Point precision: 8,8,8 or 16,16,16\n",
    "    dims=config[\"in_shape\"],                              # Image dimensions [C,H,W]\n",
    "    transpose=config[\"transpose\"],                        # Transpose argument to caffe transformer\n",
    "    channel_swap=config[\"channel_swap\"],                  # Channel swap argument to caffe transfomer\n",
    "    raw_scale=config[\"raw_scale\"],                        # Raw scale argument to caffe transformer\n",
    "    mean_value=config[\"img_mean\"],                        # Image mean per channel to caffe transformer\n",
    "    input_scale=config[\"input_scale\"],                    # Input scale argument to caffe transformer\n",
    "    calibration_size=config[\"calibration_size\"],          # Number of calibration images to use\n",
    "    calibration_directory=config[\"calibration_directory\"] # Directory containing calbration images\n",
    ")\n",
    "\n",
    "# Invoke quantizer\n",
    "try:\n",
    "    quantizer.quantize()\n",
    "\n",
    "    import json\n",
    "    data = json.loads(open(config[\"quantizecfg\"]).read())\n",
    "    print(\"**********\\nSuccessfully produced quantization JSON file for %d layers.\\n\"%len(data['network']))\n",
    "except Exception as e:\n",
    "    print(\"Failed to quantize:\",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Deploy The Model\n",
    "\n",
    "\n",
    "Next, we need to utilize the xfDNN APIs to deploy our network to the FPGA. We will walk through the deployment APIs, step by step: \n",
    "1. Open a handle for FPGA communication\n",
    "2. Load weights, biases, and quantization parameters to the FPGA DDR\n",
    "3. Allocate storage for FPGA inputs (such as images to process)\n",
    "4. Allocate storage for FPGA outputs (the activation of the final layer run on the FPGA)\n",
    "5. Execute the network\n",
    "6. Run region layer on the CPU\n",
    "7. Run non-max suppression on the CPU\n",
    "8. Print the result \n",
    "9. When you are done, close the handle to the FPGA\n",
    "\n",
    "First, we will create the handle to communicate with the FPGA and choose which FPGA overlay to run the inference on. For this lab, we will use the `xdnn_v2_32x56_2pe_16b_6mb_bank21` overlay. You can learn about other overlay options in the ML Suite Tutorials [Overlay Selector Guide][].  \n",
    "\n",
    "[Overlay Selector Guide]: https://github.com/Xilinx/ml-suite/blob/master/overlaybins/README.md\n",
    "        \n",
    "### 6. Open a handle for FPGA communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a handle with which to communicate to the FPGA\n",
    "# The actual handle is managed by xdnn\n",
    "config[\"xclbin\"] = \"../../overlaybins/\" + config[\"platform\"] + \"/overlay_3.xclbin\" # Chosen Hardware Overlay\n",
    "## NOTE: If you change the xclbin, we likely need to change some arguments provided to the compiler\n",
    "## Specifically, the DSP array width, and the memory arguments\n",
    "\n",
    "ret, handles = pyxfdnn.createHandle(config['xclbin'])\n",
    "if ret:                                                             \n",
    "    print(\"ERROR: Unable to create handle to FPGA\")\n",
    "else:\n",
    "    print(\"INFO: Successfully created handle to FPGA\")\n",
    "    \n",
    "# If this step fails, most likely the FPGA is locked by another user, or there is some setup problem with the hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Apply quantization scaling and transfer model weights to the FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize, and transfer the weights to FPGA DDR\n",
    "\n",
    "config[\"scaleA\"] = 10000 # Global scaler for weights (Must be defined)\n",
    "config[\"scaleB\"] = 30 # Global scaler for bias (Must be defined)\n",
    "config[\"PE\"] = 0 # Run on Processing Element 0 - Different xclbins have a different number of Elements\n",
    "config[\"batch_sz\"] = 1 # We will load 1 image at a time from disk\n",
    "config[\"in_shape\"] = (3,608,608) # We will resize images to 608x608\n",
    "\n",
    "#(weightsBlob, fcWeight, fcBias ) = pyxfdnn_io.loadWeights(config)\n",
    "fpgaRT = pyxfdnn.XDNNFPGAOp(handles,config)\n",
    "(fcWeight, fcBias) = pyxfdnn_io.loadFCWeightsBias(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Allocate space in host memory for inputs, load images from disk, and prepare images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate space in host memory for inputs, Load images from disk\n",
    "batch_array = np.empty(((config['batch_sz'],) + config['in_shape']), dtype=np.float32, order='C')\n",
    "img_paths = pyxfdnn_io.getFilePaths(config['images'])\n",
    "\n",
    "shapes = []\n",
    "for i in xrange(0, len(img_paths), config['batch_sz']):\n",
    "    pl = []\n",
    "    for j, p in enumerate(img_paths[i:i + config['batch_sz']]):\n",
    "        batch_array[j, ...], s = pyxfdnn_io.loadYoloImageBlobFromFile(p, config['in_shape'][2], config['in_shape'][1])\n",
    "        pl.append(p)\n",
    "        shapes.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Allocate space in host memory for outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate space in host memory for outputs\n",
    "\n",
    "# The output of a yolo network is more complex than most networks.\n",
    "\n",
    "# In YOLOv2 the input image is square, and it is divided into grid cells of 32 pixels each.\n",
    "# For example, a 608x608 image would yield a volume with HxW = 19x19. Where 19 = 608/32\n",
    "#              a 224x224 image would yield a volume with HxW = 7x7\n",
    "out_h = out_w = config[\"in_shape\"][1] / 32\n",
    "\n",
    "# For each grid cell in the input image, YOLOv2 considers the possibility of 5 objects being present (Anchor Boxes)\n",
    "#   For each anchor box, YOLOv2 computes\n",
    "#   - The probability that there is an object in the anchor box - 1 element\n",
    "#   - The x,y,w,h coordinates of a bounding box (centroid and width height of box) - 4 elements\n",
    "#   - The class probabilities (Number of classes depends on dataset) We use MS COCO which has 80 classes - 80 elements\n",
    "anchor_boxes = 5\n",
    "objectness = 1\n",
    "coordinates = 4\n",
    "classes = 80\n",
    "out_c = objectness+coordinates+classes\n",
    "\n",
    "# Number of elements in the activation of the last layer ran on the FPGA\n",
    "config[\"fpgaoutsz\"] = anchor_boxes*out_c*out_h*out_w \n",
    "\n",
    "fpgaOutput = np.empty ((config['batch_sz'], config['fpgaoutsz'],), dtype=np.float32, order='C') # Space for fpga output\n",
    "\n",
    "print (\"YOLOv2 will produce an output volume of shape (%dx%dx%dx%d) consisting of %d elements\" \\\n",
    "           %(anchor_boxes,out_c,out_h,out_w,config[\"fpgaoutsz\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Write optimized micro-code to the xDNN Processing Engine on the FPGA. Execute the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write FPGA Instructions to FPGA and Execute the network!\n",
    "startTime = timeit.default_timer()\n",
    "fpgaRT.execute(batch_array, fpgaOutput)\n",
    "elapsedTime = timeit.default_timer() - startTime\n",
    "print(\"Time to load microcode and execute %d ms\" % (elapsedTime*1000))\n",
    "    \n",
    "# Note that the time printed here is the time to transfer microcode to the FPGA + the time to execute\n",
    "# Iterative calls to this cell will produce the real latency number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Execute the YOLOv2 region layer on the CPU\n",
    "The region layer requires the sigmoid function to be applied to the X/Y coordinates of each prediction, and the box confidence. It also requires the softmax to be computed across all class scores in each anchor box. Here we use numpy to acheive this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions Required for implementing YOLOv2 region layer\n",
    "# Implemented using numpy in a numerically stable manner\n",
    "def sigmoid(x):\n",
    "    pos_mask = (x >= 0)\n",
    "    neg_mask = (x < 0)\n",
    "    z = np.zeros_like(x)\n",
    "    z[pos_mask] = np.exp(-x[pos_mask])\n",
    "    z[neg_mask] = np.exp(x[neg_mask])\n",
    "    top = np.ones_like(x)\n",
    "    top[neg_mask] = z[neg_mask]\n",
    "    return top / (1 + z)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return (e_x)/(e_x.sum(axis=0,keepdims=True))\n",
    "\n",
    "startTime = timeit.default_timer()\n",
    "# Reshape the fpgaOutputs into a 4D volume\n",
    "fpgaOutput = fpgaOutput.reshape(anchor_boxes,out_c,out_h,out_w)\n",
    "\n",
    "# Apply sigmoid to 1st, 2nd, 4th channel for all anchor boxes\n",
    "fpgaOutput[:,0:2,:,:] = sigmoid(fpgaOutput[:,0:2,:,:]) # (X,Y) Predictions\n",
    "fpgaOutput[:,4,:,:]   = sigmoid(fpgaOutput[:,4,:,:])   # Objectness / Box Confidence\n",
    "\n",
    "# Apply softmax on the class scores foreach anchor box\n",
    "for box in range(anchor_boxes):\n",
    "    fpgaOutput[box,5:,:,:]  = softmax(fpgaOutput[box,5:,:,:])\n",
    "    \n",
    "elapsedTime = timeit.default_timer() - startTime\n",
    "print(elapsedTime*1000, \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Execute NMS on the CPU\n",
    "Non-Max Suppression is the algorithim used to filter out redundant predictions. For instance, yolo could have produced multiple bounding boxes for the same object. Additionally, predictions made with low confidence are thrown out.\n",
    "We have encapsulated some C code for implementing NMS in a python wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Non-Max Suppression\n",
    "# Non-Max Suppression filters out detections with a score lesser than 0.24\n",
    "# Additionally if there are two predections with an overlap > 30%, the prediction with the lower score will be filtered\n",
    "scorethresh = 0.24\n",
    "iouthresh = 0.3\n",
    "bboxes = nms.do_baseline_nms(fpgaOutput.flat,\n",
    "                             shapes[0][1],\n",
    "                             shapes[0][0],\n",
    "                             config['in_shape'][2],\n",
    "                             config['in_shape'][1],\n",
    "                             out_w,\n",
    "                             out_h,\n",
    "                             anchor_boxes,\n",
    "                             classes,\n",
    "                             scorethresh,\n",
    "                             iouthresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Print out results\n",
    "After NMS all operations are complete. Now we must print the results, and we can draw the detections on the original image for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a list of class labels given a file containing the coco dataset classes\n",
    "with open(\"../../apps/yolo/coco.names\") as f:      \n",
    "    namez = f.readlines()      \n",
    "    names = [x.strip() for x in namez]\n",
    "    \n",
    "# Lets print the detections our model made\n",
    "for j in range(len(bboxes)):\n",
    "    print(\"Obj %d: %s\" % (j, names[bboxes[j]['classid']]))\n",
    "    print(\"\\t score = %f\" % (bboxes[j]['prob']))\n",
    "    print(\"\\t (xlo,ylo) = (%d,%d)\" % (bboxes[j]['ll']['x'], bboxes[j]['ll']['y']))\n",
    "    print(\"\\t (xhi,yhi) = (%d,%d)\" % (bboxes[j]['ur']['x'], bboxes[j]['ur']['y']))\n",
    "    \n",
    "# Given the detection results above, lets draw our findings on the original image, and display it\n",
    "colors = generate_colors(classes)\n",
    "draw_boxes(config[\"images\"][0],bboxes,names,colors,\"../work\",\"../../apps/yolo/font\",False)\n",
    "outimage = \"../work/\" + config[\"images\"][0].split(\"/\")[-1]\n",
    "img = cv2.imread(outimage)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.rcParams['figure.figsize'] = [24.0,16.0]\n",
    "plt.imshow(img)\n",
    "plt.title(\"Output Image w/ Bounding Boxes Drawn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Close the handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyxfdnn.closeHandle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You're Done! \n",
    "\n",
    "Now that you have made it this far, try selecting another image from the widget in Step 1 and click **Kernel** from the menu, and select **Restart & Run All** to see if you can classify the flower! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
