{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification w/ TensorFlow Model\n",
    "This tutorial demonstrates the steps required to prepare and deploy a trained TensorFlow model for FPGA acceleration  \n",
    "We will prepare a trained Inception v1 model, and then run a single inference.  \n",
    "\n",
    "# Model Preparation (Offline Process, Performed Once):\n",
    "\n",
    "\n",
    "## Phase 1: Compile The Model  \n",
    "    * A Network Graph (protobuf) is compiled\n",
    "    * The network is optimized\n",
    "    * FPGA Instructions are generated\n",
    "      * These instructions are required to run the network in \"one-shot\", and minimize data movement\n",
    "\n",
    "## Phase 2: Quantize The Model\n",
    "    * The Quantizer will generate a json file holding scaling parameters for quantizing floats to INT16 or INT8\n",
    "    * This is required, because FPGAs will take advantage of Fixed Point Precision, to achieve faster inference\n",
    "      * While floating point precision is useful in the model training scenario\n",
    "          It is not required for high speed, high accuracy inference\n",
    "    \n",
    "# Model Deployment (Online Process, Typically Performed Iteratively):  \n",
    "    \n",
    "## Phase 3: Deploy The Model\n",
    "Once you have the outputs of the compiler and quantizer, you will use the xfDNN deployment APIs to:\n",
    "1. Open a handle for FPGA communication\n",
    "2. Load weights, biases, and quantization parameters to the FPGA DDR\n",
    "3. Allocate storage for FPGA inputs (such as images to process)\n",
    "4. Allocate storage for FPGA outputs (the activation of the final layer run on the FPGA)\n",
    "5. Execute the network\n",
    "6. Run fully connected layers on the CPU\n",
    "7. Run Softmax on CPU\n",
    "8. Print the result (or send the result for further processing)\n",
    "9. When you are done, close the handle to the FPGA\n",
    "\n",
    "### Step 1. Import required packages, check environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some things\n",
    "import os,sys,cv2\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Bring in Xilinx ML Suite Compiler, Quantizer, PyXDNN\n",
    "from xfdnn.tools.compile.bin.xfdnn_compiler_tensorflow import TFFrontend as xfdnnCompiler\n",
    "from xfdnn.tools.quantize.quantize_tf import tf_Quantizer as xfdnnQuantizer\n",
    "import xfdnn.rt.xdnn as xdnn\n",
    "import xfdnn.rt.xdnn_io as xdnn_io\n",
    "\n",
    "import ipywidgets\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "print(\"Current working directory: %s\" % os.getcwd())\n",
    "print(\"Running on host: %s\" % os.uname()[1])\n",
    "print(\"Running w/ LD_LIBRARY_PATH: %s\" %  os.environ[\"LD_LIBRARY_PATH\"])\n",
    "print(\"Running w/ XILINX_OPENCL: %s\" %  os.environ[\"XILINX_OPENCL\"])\n",
    "print(\"Running w/ XCLBIN_PATH: %s\" %  os.environ[\"XCLBIN_PATH\"])\n",
    "print(\"Running w/ PYTHONPATH: %s\" %  os.environ[\"PYTHONPATH\"])\n",
    "print(\"Running w/ SDACCEL_INI_PATH: %s\" %  os.environ[\"SDACCEL_INI_PATH\"])\n",
    "\n",
    "id = !whoami\n",
    "\n",
    "# Make sure there is no error in this cell\n",
    "# The xfDNN runtime depends upon the above environment variables\n",
    "\n",
    "config = {} # Config dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Use a config dictionary to pass parameters.\n",
    "\n",
    "Here, we will setup and use a config dictionary to simplify handling of the arguments. For this first example, we will attempt to classify a picture of a dog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"platform\"] = None\n",
    "platforms = [\"alveo-u200\",\"alveo-u200-ml\",\"alveo-u250\",\"aws\",\"nimbix\",\"1525\",\"1525-ml\"]\n",
    "\n",
    "def setPlatform(platform):\n",
    "    global config\n",
    "    config[\"platform\"] = platform\n",
    "\n",
    "print (\"Please select your hardware platform\")\n",
    "ipywidgets.interact(setPlatform,platform=platforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Running on platform: %s\" % config[\"platform\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose an image to run, display it for reference\n",
    "config[\"images\"] = [\"../examples/classification/dog.jpg\"] # Image of interest (Must provide as a list)\n",
    "\n",
    "img = cv2.imread(config[\"images\"][0])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.title(config[\"images\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Define an xfdnnCompiler instance and pass it arguments.  \n",
    "To simplify handling of arguments, we continue to use a config dictionary. Take a look at the dictionary entries below. \n",
    "\n",
    "The arguments that need to be passed are: \n",
    "- `protobuf` - Caffe representation of the network\n",
    "- `netcfg` - Filename to save micro-instruction produced by the compiler needed to deploy\n",
    "- `memory` - Parameter to set the on-chip memory for the target xDNN overlay. This example will target an overlay with 5 MB of cache. \n",
    "- `dsp` - Parameter to set the size of the target xDNN overlay. This example uses an overlay of size 32x56 DSPs. \n",
    "- `finalnode` - Output node of the tensorflow graph  \n",
    "\n",
    "//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////  \n",
    "Memory, and DSP are critical arguments that correspond to the hardware accelerator you plan to load onto the FPGA.  \n",
    "The memory, and dsp parameters can be extracted from the name of the fpga programming file \"xclbin\".  \n",
    "Don't worry about this detail for now. Just know that if you change the xclbin, you have to recheck these parameters.  \n",
    "//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////  \n",
    "\n",
    "The xfDNN Compiler interfaces with Caffe to read a network graph, and generates a sequence of instructions for the xfDNN Deploy APIs to execute on the FPGA.  \n",
    "\n",
    "During this process the xfDNN Compiler performs computational graph traversal, node merging and optimization, memory allocation and optimization and, finally, micro-instruction generation.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiler Arguments\n",
    "config[\"model\"] = \"GoogLeNet\"\n",
    "config[\"protobuf\"] = \"../models/tensorflow/bvlc_googlenet_without_lrn/fp32/bvlc_googlenet_without_lrn_test.pb\"\n",
    "#config[\"outmodel\"] = \"work/optimized_model\" # String for naming optimized model NOT YET SUPPORTED\n",
    "config[\"netcfg\"] = \"work/fpga.cmds\" # Compiler will generate FPGA instructions\n",
    "config[\"memory\"] = 5 # Available on-chip SRAM\n",
    "config[\"dsp\"] = 56 # Width of Systolic Array\n",
    "config[\"finalnode\"] = \"prob\" # Terminal node in your tensorflow graph\n",
    "\n",
    "compiler = xfdnnCompiler(\n",
    "    networkfile=config[\"protobuf\"],      # Protobuf filename: input file\n",
    "    #anew=config[\"outmodel\"],            # String for intermediate protobuf NOT YET SUPPORTED\n",
    "    generatefile=config[\"netcfg\"],       # Script filename: output file\n",
    "    memory=config[\"memory\"],             # Available on chip SRAM within xclbin\n",
    "    dsp=config[\"dsp\"],                   # Rows in DSP systolic array within xclbin # keep defaults \n",
    "    finalnode=config[\"finalnode\"],       # Terminal node in your tensorflow graph\n",
    "    weights=True                         # Instruct Compiler to generate a weights directory for runtime\n",
    ")\n",
    "\n",
    "# Invoke compiler\n",
    "try:\n",
    "    compiler.compile()\n",
    "    \n",
    "    # The compiler extracts the floating point weights from the .caffemodel. \n",
    "    # This weights dir will be stored in the work dir with the appendex '_data'. \n",
    "    # The compiler will name it after the caffemodel, and append _data\n",
    "    config[\"datadir\"] = \"work/\" + os.path.basename(config[\"protobuf\"]) + \"_data\"\n",
    "        \n",
    "    if os.path.exists(config[\"datadir\"]) and os.path.exists(config[\"netcfg\"]+\".json\"):\n",
    "        print(\"Compiler successfully generated JSON and the data directory: {:s}\".format(config[\"datadir\"]))\n",
    "    else:\n",
    "        print(\"Compiler failed to generate the JSON or data directory: {:s}\".format(config[\"datadir\"]))\n",
    "        raise\n",
    "        \n",
    "    print(\"**********\\nCompilation Successful!\\n\")\n",
    "    \n",
    "    import json\n",
    "    data = json.loads(open(config[\"netcfg\"]+\".json\").read())\n",
    "    print(\"Network Operations Count: {:d}\".format(data['ops']))\n",
    "    print(\"DDR Transfers (bytes): {:d}\".format(data['moveops']))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Failed to complete compilation:\",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Create Quantizer Instance and run it.\n",
    "\n",
    "To simplify handling of arguments, a config dictionary is used. Take a look at the dictionary below.\n",
    "\n",
    "The arguments that need to be passed are:\n",
    "- `model_file` - Filename generated by the compiler for the optimized prototxt and caffemodel.\n",
    "- `quantizecfg` - Output JSON filename of quantization scaling parameters. \n",
    "- `bitwidths` - Desired precision from quantizer. This is to set the precision for [image data, weight bitwidth, conv output]. All three values need to be set to the same setting. The valid options are `16` for Int16 and `8` for Int8.  \n",
    "- `img_mean` - Depending on network training, subtract image mean if available.\n",
    "- `calibration_size` - Number of images the quantizer will use to calculate the dynamic range. \n",
    "- `calibration_directory` - Location of dir of images used for the calibration process. \n",
    "\n",
    "Below is an example with all the parameters filled in. `channel_swap` `raw_scale` `img_mean` `input_scale` are image preprocessing arguments specific to a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"img_mean\"] = [104.007, 116.669, 122.679] # Mean of the training set\n",
    "config[\"quantizecfg\"] = \"work/quantization_params.json\" # Quantizer will generate quantization params\n",
    "config[\"calibration_directory\"] = \"../xfdnn/tools/quantize/calibration_directory\" # Directory of images for quantizer\n",
    "config[\"calibration_size\"] = 15 # Number of calibration images quantizer will use\n",
    "config[\"bitwidths\"] = [16,16,16] # Supported quantization precision\n",
    "config[\"img_raw_scale\"] = 255.0 # Raw scale of input pixels, i.e. 0 <-> 255\n",
    "config[\"img_input_scale\"] = 1.0 # Input multiplier, Images are scaled by this factor after mean subtraction\n",
    "\n",
    "\n",
    "quantizer = xfdnnQuantizer(\n",
    "    model_file=config[\"protobuf\"],          # Prototxt filename: input file\n",
    "    quantize_config=config[\"quantizecfg\"],  # Quant filename: output file\n",
    "    bitwidths=config[\"bitwidths\"],          # Fixed Point precision: 8b or 16b\n",
    "    cal_size=config[\"calibration_size\"],    # Number of calibration images to use\n",
    "    img_mean=config[\"img_mean\"],            # Image mean per channel to caffe transformer\n",
    "    cal_dir=config[\"calibration_directory\"] # Directory containing calbration images\n",
    ")\n",
    "\n",
    "# Invoke quantizer\n",
    "try:\n",
    "    quantizer.quantize(inputName = \"data\", outputName = \"prob\")\n",
    "\n",
    "    import json\n",
    "    data = json.loads(open(config[\"quantizecfg\"]).read())\n",
    "    print(\"**********\\nSuccessfully produced quantization JSON file for %d layers.\\n\"%len(data['network']))\n",
    "except Exception as e:\n",
    "    print(\"Failed to quantize:\",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3. Deploy The Model.\n",
    "Next, we need to utilize the xfDNN APIs to deploy our network to the FPGA. We will walk through the deployment APIs, step by step: \n",
    "1. Open a handle for FPGA communication\n",
    "2. Load weights, biases, and quantization parameters to the FPGA DDR\n",
    "3. Allocate storage for FPGA inputs (such as images to process)\n",
    "4. Allocate storage for FPGA outputs (the activation of the final layer run on the FPGA)\n",
    "5. Execute the network\n",
    "6. Run fully connected layers on the CPU\n",
    "7. Run Softmax on CPU\n",
    "8. Print the result (or send the result for further processing)\n",
    "9. When you are done, close the handle to the FPGA\n",
    "\n",
    "First, we will create the handle to communicate with the FPGA and choose which FPGA overlay to run the inference on. For this lab, we will use the `xdnn_v2_32x56_2pe_16b_6mb_bank21` overlay. You can learn about other overlay options in the ML Suite Tutorials [here][].  \n",
    "\n",
    "[here]: https://github.com/Xilinx/ml-suite\n",
    "        \n",
    "### Step 5. Open a handle for FPGA communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a handle with which to communicate to the FPGA\n",
    "# The actual handle is managed by xdnn\n",
    "config[\"xclbin\"] = \"../overlaybins/\" + config[\"platform\"] + \"/overlay_3.xclbin\" # Chosen Hardware Overlay\n",
    "## NOTE: If you change the xclbin, we likely need to change some arguments provided to the compiler\n",
    "## Specifically, the DSP array width, and the memory arguments\n",
    "\n",
    "ret, handles = xdnn.createHandle(config['xclbin'])\n",
    "\n",
    "if ret:                                                             \n",
    "    print(\"ERROR: Unable to create handle to FPGA\")\n",
    "else:\n",
    "    print(\"INFO: Successfully created handle to FPGA\")\n",
    "    \n",
    "# If this step fails, most likely the FPGA is locked by another user, or there is some setup problem with the hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Apply quantization scaling and transfer model weights to the FPGA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize, and transfer the weights to FPGA DDR\n",
    "\n",
    "# config[\"datadir\"] = \"work/\" + config[\"caffemodel\"].split(\"/\")[-1]+\"_data\" # From Compiler\n",
    "config[\"scaleA\"] = 10000 # Global scaler for weights (Must be defined)\n",
    "config[\"scaleB\"] = 30 # Global scaler for bias (Must be defined)\n",
    "config[\"PE\"] = 0 # Run on Processing Element 0 - Different xclbins have a different number of Elements\n",
    "config[\"batch_sz\"] = 1 # We will load 1 image at a time from disk\n",
    "config[\"in_shape\"] = (3,224,224) # We will resize images to 224x224\n",
    "\n",
    "#(weightsBlob, fcWeight, fcBias ) = pyxfdnn_io.loadWeights(config)\n",
    "fpgaRT = xdnn.XDNNFPGAOp(handles,config)\n",
    "(fcWeight, fcBias) = xdnn_io.loadFCWeightsBias(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Allocate space in host memory for inputs, load images from disk, and prepare images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate space in host memory for inputs, Load images from disk\n",
    "batch_array = np.empty(((config['batch_sz'],) + config['in_shape']), dtype=np.float32, order='C')\n",
    "img_paths = xdnn_io.getFilePaths(config['images'])\n",
    "\n",
    "for i in xrange(0, len(img_paths), config['batch_sz']):\n",
    "    pl = []\n",
    "    for j, p in enumerate(img_paths[i:i + config['batch_sz']]):\n",
    "        batch_array[j, ...], _ = xdnn_io.loadImageBlobFromFile(p, config['img_raw_scale'], config['img_mean'], \n",
    "                                                                  config['img_input_scale'], config['in_shape'][2], \n",
    "                                                                  config['in_shape'][1])\n",
    "        pl.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Allocate space in host memory for outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate space in host memory for outputs\n",
    "if config[\"model\"] == \"GoogLeNet\":\n",
    "    config[\"fpgaoutsz\"] = 1024 # Number of elements in the activation of the last layer ran on the FPGA\n",
    "elif config[\"model\"] == \"ResNet50\":\n",
    "    config[\"fpgaoutsz\"] = 2048 # Number of elements in the activation of the last layer ran on the FPGA\n",
    "\n",
    "config[\"outsz\"] = 1000 # Number of elements output by FC layers (1000 used for imagenet)\n",
    "\n",
    "fpgaOutput = np.empty ((config['batch_sz'], config['fpgaoutsz'],), dtype=np.float32, order='C') # Space for fpga output\n",
    "fcOutput = np.empty((config['batch_sz'], config['outsz'],), dtype=np.float32, order='C') # Space for output of inner product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9. Write optimized micro-code to the xDNN Processing Engine on the FPGA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write FPGA Instructions to FPGA and Execute the network!\n",
    "fpgaRT.execute(batch_array, fpgaOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10. Execute the Fully Connected Layers on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the inner product\n",
    "xdnn.computeFC(fcWeight, fcBias, fpgaOutput, config['batch_sz'], config['outsz'], config['fpgaoutsz'], fcOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11. Execute the Softmax layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the softmax to convert the output to a vector of probabilities\n",
    "softmaxOut = xdnn.computeSoftmax(fcOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12. Output the classification prediction scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification given the labels synset_words.txt (Imagenet classes)\n",
    "config[\"labels\"] = \"../examples/classification/synset_words.txt\"\n",
    "labels = xdnn_io.get_labels(config['labels'])\n",
    "xdnn_io.printClassification(softmaxOut, pl, labels)\n",
    "\n",
    "#Print Original Image for Reference \n",
    "img = cv2.imread(config[\"images\"][0])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.title(config[\"images\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13. Close the handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdnn.closeHandle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C'est fini!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
