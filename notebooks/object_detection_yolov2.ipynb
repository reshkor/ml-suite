{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection w/ YOLOv2 (Darknet -> Caffe)\n",
    "This tutorial demonstrates the steps required to prepare and deploy a trained Darknet model for FPGA acceleration  \n",
    "We will prepare a trained YOLO v2 model, and then run a single detection.  \n",
    "\n",
    "## Introduction\n",
    "You only look once (YOLO) is a state-of-the-art, real-time object detection algorithm.  \n",
    "The algorithm was published by Redmon et al. in 2016 via the following publications:\n",
    "[YOLOv1](https://arxiv.org/abs/1506.02640),\n",
    "[YOLOv2](https://arxiv.org/abs/1612.08242).  \n",
    "The same author has already released YOLOv3, and some experimental tiny YOLO networks. We focus on YOLOv2.  \n",
    "This application requires more than just simple classification. The task here is to detect the presence of objects, and localize them within a frame.  \n",
    "Please refer to the papers for full algorithm details, and/or watch [this.](https://www.youtube.com/watch?v=9s_FpMpdYW8) \n",
    "In this tutorial, the network was trained on the 80 class [COCO dataset.](http://cocodataset.org/#home)\n",
    "\n",
    "## Background\n",
    "The authors of the YOLO papers used their own programming framework called \"Darknet\" for research, and development. \n",
    "The framework is written in C, and was [open sourced.](https://github.com/pjreddie/darknet)\n",
    "Additionally, they host documentation, and pretrained weights [here.](https://pjreddie.com/darknet/yolov2/)\n",
    "Currently, the Darknet framework is not supported by Xilinx's ML Suite.\n",
    "Additionally, there are some aspects of the YOLOv2 network that are not supported by the Hardware Accelerator, such as the leaky ReLU activation function. For these reasons the network was modified, retrained, and converted to caffe. In this tutorial we will run the network accelerated on an FPGA using 16b quantized weights and a hardware kernel implementing a 56x32 systolic array with 5MB of image RAM. All convolutions/pools are accelerated on the FPGA fabric, while the final sigmoid, softmax, and non-max suppression functions are executed on the CPU.  \n",
    "\n",
    "### Network Manual Modifications (Implemented directly in Darknet)\n",
    "To accommodate FPGA acceleration, two modifications are required for the YOLOv2 network, and then retraining must occur: \n",
    "* Leaky ReLU replaced by ReLU (The FPGA HW does not support Leaky ReLU)\n",
    "* \"reorg\" layer a.k.a. \"space_to_depth\" layer replaced by MAX POOL (W/H Dimensionality is maintained)\n",
    "  * The reorg layer does not appreciably improve accuracy, and incurrs costly data movement penalties.\n",
    "  * Converting to max pool preserves the dimensionality required for downstream layers.\n",
    "  ``` \n",
    "  # Converting below layer to maxpool to preserve downstream HxW relationships\n",
    "  # [reorg]\n",
    "  # stride=2\n",
    "  [maxpool]\n",
    "  size=2\n",
    "  stride=2\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation (Offline Process, Performed Once):\n",
    "\n",
    "## Phase 1: Convert to Caffe\n",
    "Xilinx provides a darknet2caffe.py python script  \n",
    "The script will take as arguments a darknet *.cfg file, and a darknet *.weights file, then generate a *.prototxt and a *.caffemodel.  \n",
    "This is necessary for integration into the downstream components of ML-Suite.  \n",
    "                                                                          \n",
    "\n",
    "## Phase 2: Compile The Model  \n",
    "    * A Network Graph (prototxt) and a Weights Blob (caffemodel) are compiled\n",
    "    * The network is optimized\n",
    "    * FPGA Instructions are generated\n",
    "      * These instructions are required to run the network in \"one-shot\", and minimize data movement\n",
    "\n",
    "## Phase 3: Quantize The Model\n",
    "    * The Quantizer will generate a json file holding scaling parameters for quantizing floats to INT16 or INT8\n",
    "    * This is required, because FPGAs will take advantage of Fixed Point Precision, to achieve faster inference\n",
    "      * While floating point precision is useful in the model training scenario\n",
    "          It is not required for high speed, high accuracy inference\n",
    "    \n",
    "# Model Deployment (Online Process, Typically Performed Iteratively):  \n",
    "    \n",
    "## Phase 4: Deploy The Model\n",
    "Once you have the outputs of the compiler and quantizer, you will use the xfDNN deployment APIs to:\n",
    "1. Open a handle for FPGA communication\n",
    "2. Load weights, biases, and quantization parameters to the FPGA DDR\n",
    "3. Allocate storage for FPGA inputs (such as images to process)\n",
    "4. Allocate storage for FPGA outputs (the activation of the final layer run on the FPGA)\n",
    "5. Execute the network\n",
    "6. Run the region layer on the CPU\n",
    "7. Run non-max suppression on the CPU\n",
    "8. Print the result (or send the result for further processing)\n",
    "9. When you are done, close the handle to the FPGA\n",
    " \n",
    "\n",
    "### Step 1. Import required packages, check environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,cv2,timeit\n",
    "from __future__ import print_function\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Bring in Xilinx ML-Suite Compiler, Quantizer, PyXDNN\n",
    "from xfdnn.tools.compile.bin.xfdnn_compiler_caffe import CaffeFrontend as xfdnnCompiler\n",
    "from xfdnn.tools.quantize.quantize import CaffeFrontend as xfdnnQuantizer\n",
    "import xfdnn.rt.xdnn as xdnn\n",
    "import xfdnn.rt.xdnn_io as xdnn_io\n",
    "\n",
    "# Bring in darknet2caffe functions\n",
    "from darknet2caffe import *\n",
    "\n",
    "# Bring in Non-Max Suppression \n",
    "import nms\n",
    "\n",
    "# Bring in utility for drawing boxes\n",
    "from yolo_utils import generate_colors, draw_boxes\n",
    "\n",
    "import ipywidgets\n",
    "\n",
    "# Ignore some warnings from the quantizer\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "print(\"Current working directory: %s\" % os.getcwd())\n",
    "print(\"Running on host: %s\" % os.uname()[1])\n",
    "print(\"Running w/ LD_LIBRARY_PATH: %s\" %  os.environ[\"LD_LIBRARY_PATH\"])\n",
    "print(\"Running w/ XILINX_OPENCL: %s\" %  os.environ[\"XILINX_OPENCL\"])\n",
    "print(\"Running w/ XCLBIN_PATH: %s\" %  os.environ[\"XCLBIN_PATH\"])\n",
    "print(\"Running w/ PYTHONPATH: %s\" %  os.environ[\"PYTHONPATH\"])\n",
    "print(\"Running w/ SDACCEL_INI_PATH: %s\" %  os.environ[\"SDACCEL_INI_PATH\"])\n",
    "\n",
    "id = !whoami\n",
    "\n",
    "print (\"Running as user: %s\" % id[0])\n",
    "# Make sure there is no error in this cell\n",
    "# The xfDNN runtime depends upon the above environment variables\n",
    "\n",
    "config = {} # Config dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Use a config dictionary to pass parameters. Load and image from disk.\n",
    "\n",
    "Here, we will setup and use a config dictionary to simplify handling of the arguments. For this example, we will attempt to classify a picture of a man standing on a horse. Image courtesy of openimages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"platform\"] = None\n",
    "platforms = [\"alveo-u200\",\"alveo-u200-ml\",\"alveo-u250\",\"aws\",\"nimbix\",\"1525\",\"1525-ml\"]\n",
    "\n",
    "def setPlatform(platform):\n",
    "    global config\n",
    "    config[\"platform\"] = platform\n",
    "\n",
    "print (\"Please select your hardware platform\")\n",
    "ipywidgets.interact(setPlatform,platform=platforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Running on platform: %s\" % config[\"platform\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose image to run, display it for reference\n",
    "config[\"images\"] = [\"../xfdnn/tools/quantize/calibration_directory/5904386289_924b24d75d_z.jpg\"] #(Must provide as a list)\n",
    "\n",
    "img = cv2.imread(config[\"images\"][0])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.rcParams['figure.figsize'] = [24.0,16.0]\n",
    "plt.imshow(img)\n",
    "plt.title(config[\"images\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Run darknet2caffe script.\n",
    "\n",
    "Here, we need to convert the model from darknet to caffe. The below cell will create a prototxt, and a caffemodel in the ./work directory. Such a translation will be necessary for any model defined in darknet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the darknet2caffe python script which will convert a yolov2 darknet model to a caffe model\n",
    "# This script additionally will remove batchnorm layers, by merging the parameters\n",
    "# This step is intentionally performed upstream of the compiler due to Darknet's non-conforming implementation of \n",
    "#   batch normalization\n",
    "\n",
    "config[\"prototxt\"] = \"./work/yolov2.prototxt\" \n",
    "config[\"caffemodel\"] = \"./work/yolov2.caffemodel\"\n",
    "\n",
    "darknet2caffe(\"../models/darknet/yolov2/fp32/yolo.xdnn.nobn.608.cfg\",\n",
    "              \"../models/darknet/yolov2/fp32/yolo.xdnn.nobn.weights\",\n",
    "              config[\"prototxt\"],\n",
    "              config[\"caffemodel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Define an xfdnnCompiler instance and pass it arguments.  \n",
    "To simplify handling of arguments, we continue to use a config dictionary. Take a look at the dictionary entries below. \n",
    "\n",
    "The arguments that need to be passed are: \n",
    "- `prototxt` - Caffe representation of the network\n",
    "- `caffemodel` - Pre-trained Model for the network \n",
    "- `outmodel` - Filename for saving the prototxt and caffemodel of the optimized network\n",
    "- `netcfg` - Filename to save micro-instruction produced by the compiler needed to deploy\n",
    "- `memory` - Parameter to set the on-chip memory for the target xDNN overlay. This example will target an overlay with 5 MB of cache. \n",
    "- `dsp` - Parameter to set the size of the target xDNN overlay. This example uses an overlay of size 32x56 DSPs. \n",
    "\n",
    "//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////  \n",
    "Memory, and DSP are critical arguments that correspond to the hardware accelerator you plan to load onto the FPGA.  \n",
    "The memory, and dsp parameters can be extracted from the name of the fpga programming file \"xclbin\".  \n",
    "Don't worry about this detail for now. Just know that if you change the xclbin, you have to recheck these parameters.  \n",
    "//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////  \n",
    "\n",
    "The xfDNN Compiler interfaces with Caffe to read a network graph, and generates a sequence of instructions for the xfDNN Deploy APIs to execute on the FPGA.  \n",
    "\n",
    "During this process the xfDNN Compiler performs computational graph traversal, node merging and optimization, memory allocation and optimization and, finally, micro-instruction generation.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiler Arguments\n",
    "#config[\"prototxt\"] = \"./work/yolov2.prototxt\" \n",
    "#config[\"caffemodel\"] = \"./work/yolov2.caffemodel\"\n",
    "config[\"outmodel\"] = \"work/optimized_model\" # String for naming optimized prototxt, caffemodel\n",
    "config[\"netcfg\"] = \"work/fpga.cmds\" # Compiler will generate FPGA instructions\n",
    "config[\"memory\"] = 5 # Available on-chip SRAM\n",
    "config[\"dsp\"] = 56 # Width of Systolic Array\n",
    "\n",
    "compiler = xfdnnCompiler(\n",
    "    networkfile=config[\"prototxt\"],       # Prototxt filename: input file\n",
    "    weights=config[\"caffemodel\"],         # Floating Point Weights: input file\n",
    "    anew=config[\"outmodel\"],              # String for intermediate prototxt/caffemodel\n",
    "    generatefile=config[\"netcfg\"],  # Script filename: output file\n",
    "    memory=config[\"memory\"],              # Available on chip SRAM within xclbin\n",
    "    dsp=config[\"dsp\"]                     # Rows in DSP systolic array within xclbin\n",
    ")\n",
    "\n",
    "# Invoke compiler\n",
    "try:\n",
    "    compiler.compile()\n",
    "\n",
    "    # The compiler extracts the floating point weights from the .caffemodel. \n",
    "    # This weights dir will be stored in the work dir with the appendex '_data'. \n",
    "    # The compiler will name it after the caffemodel, and append _data\n",
    "    config[\"datadir\"] = \"work/\" + config[\"caffemodel\"].split(\"/\")[-1]+\"_data\"\n",
    "        \n",
    "    if os.path.exists(config[\"datadir\"]) and os.path.exists(config[\"netcfg\"]+\".json\"):\n",
    "        print(\"Compiler successfully generated JSON and the data directory: %s\" % config[\"datadir\"])\n",
    "    else:\n",
    "        print(\"Compiler failed to generate the JSON or data directory: %s\" % config[\"datadir\"])\n",
    "        raise\n",
    "        \n",
    "    print(\"**********\\nCompilation Successful!\\n\")\n",
    "    \n",
    "    import json\n",
    "    data = json.loads(open(config[\"netcfg\"]+\".json\").read())\n",
    "    print(\"Network Operations Count: %d\"%data['ops'])\n",
    "    print(\"DDR Transfers (bytes): %d\"%data['moveops']) \n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Failed to complete compilation:\",e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Create Quantizer Instance and pass arguments.\n",
    "\n",
    "To simplify handling of arguments, a config dictionary is used. Take a look at the dictionary below.\n",
    "\n",
    "The arguments that need to be passed are:\n",
    "- `deploy_model` - Filename generated by the compiler for the optimized prototxt and caffemodel.\n",
    "- `quantizecfg` - Output JSON filename of quantization scaling parameters. \n",
    "- `bitwidths` - Desired precision from quantizer. This is to set the precision for [image data, weight bitwidth, conv output]. All three values need to be set to the same setting. The valid options are `16` for Int16 and `8` for Int8.\n",
    "- `transpose` - Images start as H,W,C (H=0,W=1,C=2) transpose swaps to C,H,W (2,0,1) for typical networks.\n",
    "- `channel_swap` - Depending on network training and image read, can swap from RGB (R=0,G=1,B=2) to BGR (2,1,0).\n",
    "- `raw_scale` - Depending on network training, scale pixel values before mean subtraction.\n",
    "- `img_mean` - Depending on network training, subtract image mean if available.\n",
    "- `input_scale` - Depending on network training, scale after subtracting mean.\n",
    "- `calibration_size` - Number of images the quantizer will use to calculate the dynamic range. \n",
    "- `calibration_directory` - Location of dir of images used for the calibration process. \n",
    "\n",
    "Below is an example with all the parameters filled in. `channel_swap` `raw_scale` `img_mean` `input_scale` are image preprocessing arguments specific to a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a config dictionary to pass parameters to the compiler\n",
    "# Quantizer Arguments\n",
    "#config[\"outmodel\"] = Defined in Step 4 # String for naming intermediate prototxt, caffemodel\n",
    "config[\"quantizecfg\"] = \"work/quantization_params.json\" # Quantizer will generate quantization params\n",
    "config[\"bitwidths\"] = [16,16,16] # Supported quantization precision\n",
    "config[\"transpose\"] = [2,0,1] # (H,W,C)->(C,H,W) transpose argument to quantizer\n",
    "config[\"channel_swap\"] = [2,1,0] # (R,G,B)->(B,G,R) Channel Swap argument to quantizer\n",
    "config[\"img_raw_scale\"] = 1.0 # Yolo effectively divides by 255 (Quantizer will represent image as 0 <-> 1 which is equiv)\n",
    "config[\"img_mean\"] = [0.0, 0.0, 0.0] # Mean of the training set used for mean subtraction (Not performed for yolo)\n",
    "config[\"img_input_scale\"] = 1.0 # Input multiplier, Images are scaled by this factor after mean subtraction\n",
    "config[\"calibration_size\"] = 15 # Number of calibration images quantizer will use\n",
    "config[\"calibration_directory\"] = \"../xfdnn/tools/quantize/calibration_directory\" # Directory of images\n",
    "\n",
    "quantizer = xfdnnQuantizer(\n",
    "    deploy_model=config[\"outmodel\"]+\".prototxt\",        # Model filename: input file\n",
    "    weights=config[\"outmodel\"]+\".caffemodel\",           # Floating Point weights\n",
    "    output_json=config[\"quantizecfg\"],                    # Quantization JSON output filename\n",
    "    bitwidths=config[\"bitwidths\"],                        # Fixed Point precision: 8,8,8 or 16,16,16\n",
    "    transpose=config[\"transpose\"],                        # Transpose argument to caffe transformer\n",
    "    channel_swap=config[\"channel_swap\"],                  # Channel swap argument to caffe transfomer\n",
    "    raw_scale=config[\"img_raw_scale\"],                        # Raw scale argument to caffe transformer\n",
    "    mean_value=config[\"img_mean\"],                        # Image mean per channel to caffe transformer\n",
    "    input_scale=config[\"img_input_scale\"],                    # Input scale argument to caffe transformer\n",
    "    calibration_size=config[\"calibration_size\"],          # Number of calibration images to use\n",
    "    calibration_directory=config[\"calibration_directory\"] # Directory containing calbration images\n",
    ")\n",
    "\n",
    "# Invoke quantizer\n",
    "try:\n",
    "    quantizer.quantize()\n",
    "\n",
    "    import json\n",
    "    data = json.loads(open(config[\"quantizecfg\"]).read())\n",
    "    print(\"**********\\nSuccessfully produced quantization JSON file for %d layers.\\n\"%len(data['network']))\n",
    "except Exception as e:\n",
    "    print(\"Failed to quantize:\",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4. Deploy The Model.\n",
    "Next, we need to utilize the xfDNN APIs to deploy our network to the FPGA. We will walk through the deployment APIs, step by step: \n",
    "1. Open a handle for FPGA communication\n",
    "2. Load weights, biases, and quantization parameters to the FPGA DDR\n",
    "3. Allocate storage for FPGA inputs (such as images to process)\n",
    "4. Allocate storage for FPGA outputs (the activation of the final layer run on the FPGA)\n",
    "5. Execute the network\n",
    "6. Run region layer on the CPU\n",
    "7. Run non-max suppression on the CPU\n",
    "8. Print the result \n",
    "9. When you are done, close the handle to the FPGA\n",
    "\n",
    "First, we will create the handle to communicate with the FPGA and choose which FPGA overlay to run the inference on. For this lab, we will use overlay_3. You can learn about other overlay options in the ML Suite Tutorials [here][].  \n",
    "\n",
    "[here]: https://github.com/Xilinx/ml-suite\n",
    "        \n",
    "### Step 6. Open a handle for FPGA communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a handle with which to communicate to the FPGA\n",
    "# The actual handle is managed by xdnn\n",
    "config[\"xclbin\"] = \"../overlaybins/\" + config[\"platform\"] + \"/overlay_3.xclbin\" # Chosen Hardware Overlay\n",
    "## NOTE: If you change the xclbin, we likely need to change some arguments provided to the compiler\n",
    "## Specifically, the DSP array width, and the memory arguments\n",
    "\n",
    "ret, handles = xdnn.createHandle(config['xclbin'])\n",
    "\n",
    "if ret:                                                             \n",
    "    print(\"ERROR: Unable to create handle to FPGA\")\n",
    "else:\n",
    "    print(\"INFO: Successfully created handle to FPGA\")\n",
    "    \n",
    "# If this step fails, most likely the FPGA is locked by another user, or there is some setup problem with the hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Apply quantization scaling and transfer model weights to the FPGA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize, and transfer the weights to FPGA DDR\n",
    "\n",
    "config[\"scaleA\"] = 10000 # Global scaler for weights (Must be defined)\n",
    "config[\"scaleB\"] = 30 # Global scaler for bias (Must be defined)\n",
    "config[\"PE\"] = 0 # Run on Processing Element 0 - Different xclbins have a different number of Elements\n",
    "config[\"batch_sz\"] = 1 # We will load 1 image at a time from disk\n",
    "config[\"in_shape\"] = (3,608,608) # We will resize images to 608x608\n",
    "\n",
    "#(weightsBlob, fcWeight, fcBias ) = pyxfdnn_io.loadWeights(config)\n",
    "fpgaRT = xdnn.XDNNFPGAOp(handles,config)\n",
    "(fcWeight, fcBias) = xdnn_io.loadFCWeightsBias(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Allocate space in host memory for inputs, load images from disk, and prepare images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate space in host memory for inputs, Load images from disk\n",
    "batch_array = np.empty(((config['batch_sz'],) + config['in_shape']), dtype=np.float32, order='C')\n",
    "img_paths = xdnn_io.getFilePaths(config['images'])\n",
    "\n",
    "shapes = []\n",
    "for i in xrange(0, len(img_paths), config['batch_sz']):\n",
    "    pl = []\n",
    "    for j, p in enumerate(img_paths[i:i + config['batch_sz']]):\n",
    "        batch_array[j, ...], s = xdnn_io.loadYoloImageBlobFromFile(p, config['in_shape'][2], config['in_shape'][1])\n",
    "        pl.append(p)\n",
    "        shapes.append(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9. Allocate space in host memory for outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate space in host memory for outputs\n",
    "\n",
    "# The output of a yolo network is more complex than most networks.\n",
    "\n",
    "# In YOLOv2 the input image is square, and it is divided into grid cells of 32 pixels each.\n",
    "# For example, a 608x608 image would yield a volume with HxW = 19x19. Where 19 = 608/32\n",
    "#              a 224x224 image would yield a volume with HxW = 7x7\n",
    "out_h = out_w = config[\"in_shape\"][1] / 32\n",
    "\n",
    "# For each grid cell in the input image, YOLOv2 considers the possibility of 5 objects being present (Anchor Boxes)\n",
    "#   For each anchor box, YOLOv2 computes\n",
    "#   - The probability that there is an object in the anchor box - 1 element\n",
    "#   - The x,y,w,h coordinates of a bounding box (centroid and width height of box) - 4 elements\n",
    "#   - The class probabilities (Number of classes depends on dataset) We use MS COCO which has 80 classes - 80 elements\n",
    "anchor_boxes = 5\n",
    "objectness = 1\n",
    "coordinates = 4\n",
    "classes = 80\n",
    "out_c = objectness+coordinates+classes\n",
    "\n",
    "# Number of elements in the activation of the last layer ran on the FPGA\n",
    "config[\"fpgaoutsz\"] = anchor_boxes*out_c*out_h*out_w \n",
    "\n",
    "fpgaOutput = np.empty ((config['batch_sz'], config['fpgaoutsz'],), dtype=np.float32, order='C') # Space for fpga output\n",
    "\n",
    "print (\"YOLOv2 will produce an output volume of shape (%dx%dx%dx%d) consisting of %d elements\" \\\n",
    "           %(anchor_boxes,out_c,out_h,out_w,config[\"fpgaoutsz\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10. Write optimized micro-code to the xDNN Processing Engine on the FPGA. Execute the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write FPGA Instructions to FPGA and Execute the network!\n",
    "startTime = timeit.default_timer()\n",
    "fpgaRT.execute(batch_array, fpgaOutput)\n",
    "elapsedTime = timeit.default_timer() - startTime\n",
    "print(\"Time to load microcode and execute %d ms\" % (elapsedTime*1000))\n",
    "    \n",
    "# Note that the time printed here is the time to transfer microcode to the FPGA + the time to execute\n",
    "# Iterative calls to this cell will produce the real latency number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11. Execute the YOLOv2 region layer on the CPU.\n",
    "The region layer requires the sigmoid function to be applied to the X/Y coordinates of each prediction, and the box confidence. It also requires the softmax to be computed across all class scores in each anchor box. Here we use numpy to acheive this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions Required for implementing YOLOv2 region layer\n",
    "# Implemented using numpy in a numerically stable manner\n",
    "def sigmoid(x):\n",
    "    pos_mask = (x >= 0)\n",
    "    neg_mask = (x < 0)\n",
    "    z = np.zeros_like(x)\n",
    "    z[pos_mask] = np.exp(-x[pos_mask])\n",
    "    z[neg_mask] = np.exp(x[neg_mask])\n",
    "    top = np.ones_like(x)\n",
    "    top[neg_mask] = z[neg_mask]\n",
    "    return top / (1 + z)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return (e_x)/(e_x.sum(axis=0,keepdims=True))\n",
    "\n",
    "startTime = timeit.default_timer()\n",
    "# Reshape the fpgaOutputs into a 4D volume\n",
    "fpgaOutput = fpgaOutput.reshape(anchor_boxes,out_c,out_h,out_w)\n",
    "\n",
    "# Apply sigmoid to 1st, 2nd, 4th channel for all anchor boxes\n",
    "fpgaOutput[:,0:2,:,:] = sigmoid(fpgaOutput[:,0:2,:,:]) # (X,Y) Predictions\n",
    "fpgaOutput[:,4,:,:]   = sigmoid(fpgaOutput[:,4,:,:])   # Objectness / Box Confidence\n",
    "\n",
    "# Apply softmax on the class scores foreach anchor box\n",
    "for box in range(anchor_boxes):\n",
    "    fpgaOutput[box,5:,:,:]  = softmax(fpgaOutput[box,5:,:,:])\n",
    "    \n",
    "elapsedTime = timeit.default_timer() - startTime\n",
    "print(elapsedTime*1000, \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12. Execute NMS on the CPU.\n",
    "Non-Max Suppression is the algorithim used to filter out redundant predictions. For instance, yolo could have produced multiple bounding boxes for the same object. Additionally, predictions made with low confidence are thrown out.\n",
    "We have encapsulated some C code for implementing NMS in a python wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Non-Max Suppression\n",
    "# Non-Max Suppression filters out detections with a score lesser than 0.24\n",
    "# Additionally if there are two predections with an overlap > 30%, the prediction with the lower score will be filtered\n",
    "scorethresh = 0.24\n",
    "iouthresh = 0.3\n",
    "bboxes = nms.do_baseline_nms(fpgaOutput.flat,\n",
    "                             shapes[0][1],\n",
    "                             shapes[0][0],\n",
    "                             config['in_shape'][2],\n",
    "                             config['in_shape'][1],\n",
    "                             out_w,\n",
    "                             out_h,\n",
    "                             anchor_boxes,\n",
    "                             classes,\n",
    "                             scorethresh,\n",
    "                             iouthresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13. Print out results.\n",
    "After NMS all operations are complete. Now we must print the results, and we can draw the detections on the original image for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of class labels given a file containing the coco dataset classes\n",
    "with open(\"../apps/yolo/coco.names\") as f:      \n",
    "    namez = f.readlines()      \n",
    "    names = [x.strip() for x in namez]\n",
    "    \n",
    "# Lets print the detections our model made\n",
    "for j in range(len(bboxes)):\n",
    "    print(\"Obj %d: %s\" % (j, names[bboxes[j]['classid']]))\n",
    "    print(\"\\t score = %f\" % (bboxes[j]['prob']))\n",
    "    print(\"\\t (xlo,ylo) = (%d,%d)\" % (bboxes[j]['ll']['x'], bboxes[j]['ll']['y']))\n",
    "    print(\"\\t (xhi,yhi) = (%d,%d)\" % (bboxes[j]['ur']['x'], bboxes[j]['ur']['y']))\n",
    "    \n",
    "# Given the detection results above, lets draw our findings on the original image, and display it\n",
    "colors = generate_colors(classes)\n",
    "draw_boxes(config[\"images\"][0],bboxes,names,colors,\"./work\",\"../apps/yolo/font\",False)\n",
    "outimage = \"./work/\" + os.path.basename(config[\"images\"][0])\n",
    "img = cv2.imread(outimage)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.rcParams['figure.figsize'] = [24.0,16.0]\n",
    "plt.imshow(img)\n",
    "plt.title(\"Output Image w/ Bounding Boxes Drawn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14. Close the handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdnn.closeHandle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C'est fini!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
